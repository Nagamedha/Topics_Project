{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Understanding the problem\n",
    "\n",
    "## 1.1 Mission\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Requirements: Libraries used in this notebook\n",
    "\n",
    "- See [`requirements.txt`] (./ requirements.txt) for the versions of the libraries tested with this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this notebook does not work with the versions\n",
    "# libraries in your environment, then\n",
    "# Decommentarize The Following Line for Tested Versions:\n",
    "\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_libraries(required={}) -> None:\n",
    "\n",
    "    import sys\n",
    "    import subprocess\n",
    "    import pkg_resources\n",
    "    installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "    missing = set(required) - set(installed)\n",
    "    if missing:\n",
    "        print(f'missing libraries: {missing}')\n",
    "        python = sys.executable\n",
    "        subprocess.check_call([python, '-m', 'pip', 'install', *missing],\n",
    "                              stdout=subprocess.DEVNULL)\n",
    "\n",
    "\n",
    "required_libraries = {'numpy', 'pandas',\n",
    "                      'matplotlib', 'seaborn', 'scikit-learn',\n",
    "                      'nltk',\n",
    "                      'gensim',\n",
    "                      'tensorflow',\n",
    "                      'transformers',\n",
    "                      'tensorflow_hub',\n",
    "                      'tensorflow_text',\n",
    "                      'wordcloud',\n",
    "                      'plotly',\n",
    "                      'kaleido'}\n",
    "install_libraries(required_libraries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "import transformers\n",
    "import tensorflow_hub\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn import metrics\n",
    "from sklearn import cluster\n",
    "from sklearn import manifold, decomposition\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing, pipeline\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import scipy\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "import plotly.io as pio\n",
    "import IPython\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 List of versions of the libraries used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "python_version()\n",
    "print('versions of libraries used:')\n",
    "print('; '.join(f'{m.__name__}=={m.__version__}' for m in globals(\n",
    ").values() if getattr(m, '__version__', None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Configuration of display defects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"white\", context=\"notebook\")\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.set_palette(\"tab20\")\n",
    "\n",
    "set_config(display='diagram')\n",
    "# Displays HTML Representation in a jupyter Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 some constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE RANDOM_SEED = None for Variable Results\n",
    "# Here we define random_seed = constant only for reproducibility\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Utility functions\n",
    "\n",
    "### 1.3.1 Graphics recording\n",
    "\n",
    "To save the graphics, define ** `save_images = true` **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_IMAGES = True\n",
    "IMAGE_FOLDER = './images/analyse'\n",
    "if not os.path.exists(IMAGE_FOLDER):\n",
    "    os.makedirs(IMAGE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize(fig_name: str) -> str:\n",
    "\n",
    "    return fig_name.replace(' ', '_').replace(':', '-').replace(\n",
    "        '.', '-').replace('/', '_').replace('>', 'gt.').replace('<', 'lt.')\n",
    "\n",
    "\n",
    "def to_png(fig_name=None) -> None:\n",
    "\n",
    "\n",
    "    def get_title() -> str:\n",
    "\n",
    "        if plt.gcf()._suptitle is None:\n",
    "            return plt.gca().get_title()\n",
    "        else:\n",
    "            return plt.gcf()._suptitle.get_text()\n",
    "\n",
    "    if SAVE_IMAGES:\n",
    "        if fig_name is None:\n",
    "            fig_name = get_title()\n",
    "        elif len(fig_name) < 9:\n",
    "            fig_name = f'{fig_name}_{get_title()}'\n",
    "        fig_name = sanitize(fig_name)\n",
    "        print(f'\"{fig_name}.png\"')\n",
    "        plt.gcf().savefig(\n",
    "            f'{IMAGE_FOLDER}/{fig_name}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_FOLDER = 'data/out'\n",
    "\n",
    "\n",
    "def save_pickle(obj, filename, filepath=OUT_FOLDER):\n",
    "    with open(f'{filepath}/{filename}.pickle', 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_pickle(filename, filepath=OUT_FOLDER):\n",
    "    with open(f'{filepath}/{filename}.pickle', 'rb') as handle:\n",
    "        return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Check that the columns are in the dataframe\n",
    "\n",
    "- without changing the order of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cols_in_df(df: pd.DataFrame, colonnes: list = None) -> list:\n",
    "\n",
    "    ret_cols = []\n",
    "    for col in colonnes:\n",
    "        if col in df.columns:\n",
    "            ret_cols.append(col)\n",
    "    return ret_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import, cleaning and exploratory analysis of data\n",
    "\n",
    "A first articles database with the photo and an associated description:\n",
    "[The link to download] (https://s3-eu-west-1.amazonaws.com/static.oc-statatic.com/prod/courses/files/parcours_data_scientist/projet+-+textimage+das+v2/dataset+Project+Pre%CC%81Trament+texts+images.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Import of data\n",
    "\n",
    "Once downloaded, data from the Zip file (329 MB) is extracted in the folder defined by Data_Folder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = 'local'\n",
    "\n",
    "if ENV == 'local':\n",
    "# # Local Development\n",
    "    DATA_FOLDER = 'data/raw'\n",
    "    OUT_FOLDER = 'data/out'\n",
    "    IMAGE_FOLDER = 'images/textes'\n",
    "\n",
    "if ENV == 'colab':\n",
    "# # COLABORATORY - Uncomment the Following 2 Lines to Connect to your Drive\n",
    "# # from Google.colab Import Drive\n",
    "# # Drive.ment ('/Content/Drive')\n",
    "    DATA_FOLDER = '/content/drive/MyDrive/data/OC6'\n",
    "    OUT_FOLDER = '/content/drive/MyDrive/data/OC6'\n",
    "    IMAGE_FOLDER = '/content/drive/MyDrive/images/OC6/nettoyage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Assuming your raw data filename and folder are defined as below\n",
    "RAW_DATA_FILENAME = '/content/data/raw/flipkart_com-ecommerce_sample_1050.csv'\n",
    "DATA_FOLDER = '/content'  # Modify this path as per your actual data folder location\n",
    "\n",
    "def os_make_dir(folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "def os_path_join(folder, file):\n",
    "    return os.path.join(folder, file)  # Using os.path.join for better path handling\n",
    "\n",
    "IMAGE_FOLDER = os_path_join(DATA_FOLDER, 'images')  # Assuming you want to store images in this folder\n",
    "os_make_dir(IMAGE_FOLDER)\n",
    "RAW_DATA = os_path_join(DATA_FOLDER, RAW_DATA_FILENAME)\n",
    "\n",
    "print(f'Data file: {RAW_DATA}')\n",
    "\n",
    "def read_file_rows(file_path=RAW_DATA, nb_rows=1):\n",
    "    with open(file_path, encoding='UTF-8') as fp:\n",
    "        for i in range(nb_rows + 1):\n",
    "            row = fp.readline()\n",
    "            print(f'Line {i} :\\n{row}')\n",
    "\n",
    "read_file_rows(RAW_DATA, nb_rows=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data format:\n",
    "\n",
    "- The first line contains the headers (Column Names)\n",
    "- The data seems separated by comma\n",
    "- We use `encoding = utf-8 '\n",
    "- The column `Product_SPecifications` is made up of Key Values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(RAW_DATA, sep=',', header=0, encoding='UTF-8')\n",
    "df_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data\n",
    "\n",
    "There is little missing data (see DF_Data.info () non-Null Count):\n",
    "\n",
    "- 1 price not indicated\n",
    "- 25% of the data do not have a brand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(df: pd.DataFrame):\n",
    "    if 'brand' in df.columns:\n",
    "        df = df.copy()\n",
    "        df['brand'] = df['brand'].fillna('missing')\n",
    "    return df\n",
    "\n",
    "\n",
    "df_cleaned = df_data.pipe(fill_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data.duplicated(subset=['uniq_id']).sum())\n",
    "print(df_data.duplicated(subset=['pid']).sum())\n",
    "print(df_data.duplicated(subset=['product_name']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['uniq_id', 'pid', 'product_name']:\n",
    "    print(f'max_chars [{col}] = {df_data[col].map(len).max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `pid` as product identifier, deleting` uniq_id`\n",
    "The `product_name` is unique in this sample but not necessarily in the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Exploratory analysis\n",
    "\n",
    "Analysis of the relevance of columns for classification:\n",
    "\n",
    "- categories\n",
    "- Brands\n",
    "- Product names and descriptions\n",
    "- sample dates\n",
    "- the costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 The categories\n",
    "\n",
    "The objective is to be able to classify the products in the product_category_tree `(or another classification) automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['product_category_tree'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data['product_category_tree'].nunique())\n",
    "print(df_data['product_category_tree'].str.lower().nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 1000 samples, it will be difficult to automate the classification in 640 categories.\n",
    "\n",
    "We look at the higher levels of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_count(row):\n",
    "\n",
    "    return row.count('[')\n",
    "\n",
    "\n",
    "df_data['product_category_tree'].map(lambda x: get_category_count(x)).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each product is associated with only one category in the Category Tree\n",
    "- The column `Product_Category_tree` is a tree, separated by '>>'.\n",
    "\n",
    "#### What is the depth of the tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth(categorie: str) -> int:\n",
    "\n",
    "    return len(categorie.split(' >> '))\n",
    "\n",
    "\n",
    "df_data['product_category_tree'].map(lambda x: get_depth(x)).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Products have up to 7 category levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_data['product_category_tree'].str.lstrip(\n",
    "    '[\"').str.rstrip('\"]').str.split(' >>', expand=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_level_1 = (df_data['product_category_tree']\n",
    "                 .str.lstrip('[\"').str.rstrip('\"]')\n",
    "                 .str.split(' >>', expand=True)).iloc[:, 0]\n",
    "categ_level_1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_brands(df: pd.DataFrame):\n",
    "    nb_brands = df['brand'].nunique()\n",
    "    df['brand'].value_counts(normalize=False).cumsum(\n",
    "    ).reset_index(drop=True).plot(kind='line')\n",
    "    ax = plt.gca()\n",
    "# # ax.tick_params (labelbottom = false)\n",
    "    ax.set_xlabel('brand')\n",
    "    ax.set_ylabel('cumulative count')\n",
    "    plt.title(f'Cumulative brand count (name unique = {nb_brands})')\n",
    "\n",
    "\n",
    "plot_brands(df_data)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_freq(df: pd.DataFrame, col, nb=10, others=True, normalize=False):\n",
    "\n",
    "    nb = max(1, nb)\n",
    "    counts_df = (df[col].value_counts(normalize=normalize)\n",
    "                 .to_frame(name='freq')\n",
    "                 .rename_axis(col)\n",
    "                 )\n",
    "    nb = min(nb, len(counts_df))\n",
    "    top_n = counts_df.head(nb).copy()\n",
    "    if others:\n",
    "        top_n.loc['other', 'freq'] = counts_df.iloc[nb:, 0].sum()\n",
    "    return top_n.reset_index()\n",
    "\n",
    "\n",
    "def plot_bar_top_n(df: pd.DataFrame, col, nb=20, others=True, normalize=False, sort_values=False, palette=None,\n",
    "                   ylabel=None, titre='', subtitle='', figsize=None):\n",
    "\n",
    "    data = top_n_freq(df, col, nb, others, normalize).copy()\n",
    "# # print (data.columns.to_list ())\n",
    "    ax = None\n",
    "    if not figsize is None:\n",
    "        _, ax = plt.subplots(figsize=figsize)\n",
    "    other_count = 0\n",
    "    if others:\n",
    "        filter_other = data[col] == 'other'\n",
    "        other_count = data[filter_other]['freq'].values.sum()\n",
    "        data = data[~filter_other]\n",
    "    if sort_values:\n",
    "        data = data.sort_values(by=col)\n",
    "    if normalize:\n",
    "        ax = sns.barplot(y=data[col], x=data['freq']\n",
    "                         * 100, palette=palette, ax=ax)\n",
    "        ax.set_xlabel('frequence (%)')\n",
    "    else:\n",
    "        ax = sns.barplot(y=data[col], x=data['freq'], palette=palette, ax=ax)\n",
    "        ax.set_xlabel(\"number of occurrences\")\n",
    "\n",
    "    autres = ''\n",
    "    if others and (other_count > 0):\n",
    "        if normalize:\n",
    "            other_count = f'{other_count * 100:.2f} %'\n",
    "        else:\n",
    "            other_count = f'{int(other_count)}'\n",
    "        autres = f' [Other values = {other_count}]'\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel)\n",
    "    sns.despine()\n",
    "    if len(titre) > 0:\n",
    "        plt.suptitle(titre, y=1.05)\n",
    "    plt.title(f'{subtitle} {autres}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "plot_bar_top_n(df_data, 'brand', nb=20,\n",
    "               subtitle=\"Top 20 brands of the sample\")\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[['brand']].join(categ_level_1).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the brand will help the classification (Netgear, Asus, HP will be classified in the 'Computers' category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Product name, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['product_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.loc[0:3, ['brand', 'product_name', 'description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the brand 'and `product_name` are included in the description`:\n",
    "\n",
    "- These 2 fields can be deleted\n",
    "- We keep them to see if we can categorize only based on brand or product name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Product specials\n",
    "\n",
    "The column `product_spercifications` contains key-value peers for product specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.loc[0, ['product_specifications']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "\n",
    "def get_spec_keys(spec_text):\n",
    "    try:\n",
    "        items = json.loads(spec_text)\n",
    "        return [item['key'] for item in items]\n",
    "    except:\n",
    "        return np.NaN\n",
    "\n",
    "\n",
    "specification_keys = (df_data['product_specifications']\n",
    "                      .str.replace('=>', ':')\n",
    "                      .str.lstrip('{\"product_specification\":')\n",
    "                      .str.rstrip('}')\n",
    "                      .map(lambda x: get_spec_keys(x))\n",
    "                      .rename('specification_keys')\n",
    "                      )\n",
    "\n",
    "spec_keys = specification_keys.explode().to_frame()\n",
    "\n",
    "print(\n",
    "    f\"number of unique specification_keys : {spec_keys['specification_keys'].nunique()}\")\n",
    "\n",
    "plot_bar_top_n(spec_keys, 'specification_keys',\n",
    "               subtitle='Top Specification Keys')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the `Value` of each Key-Value Pair are included in the field` Description`\n",
    "\n",
    "- So we can delete this field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 Sample dates (crawl_timestamp)\n",
    "\n",
    "- Is the data comparable (they come from the same dates)?\n",
    "- Samples come from a period of 7 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=pd.to_datetime(\n",
    "    df_data['crawl_timestamp']).dt.strftime('%Y_%m').sort_values())\n",
    "sns.despine()\n",
    "plt.title('Data date distribution')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6 Price distribution\n",
    "\n",
    "The columns `Retail_price` and` Discouned_price` can help classify products between categories, because we are waiting for healthcare products to be cheaper than computers, for example\n",
    "\n",
    "Nevertheless, as data comes from different dates, we avoid using the price for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_level_1 = df_data['product_category_tree'].str.lstrip(\n",
    "    '[\"').str.rstrip('\"]').str.split(' >>', expand=True).iloc[:, 0]\n",
    "\n",
    "sns.histplot(data=df_data, x='retail_price', hue=categ_level_1,\n",
    "             palette='nipy_spectral',\n",
    "             alpha=0.2,\n",
    "             log_scale=True,\n",
    "                     kde=True)\n",
    "sns.despine()\n",
    "plt.title(f'Price distribution by category (logarithmic scale)')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Elimination of columns not relevant to the problem\n",
    "\n",
    "Before doing feature engineering, the unused columns are deleted for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unused_columns(df):\n",
    "\n",
    "    colonnes_non_pertinentes = ['uniq_id', 'crawl_timestamp', 'product_url', 'retail_price', 'discounted_price',\n",
    "                                'is_FK_Advantage_product', 'product_rating', 'overall_rating', 'product_specifications']\n",
    "    cols_to_drop = cols_in_df(df_cleaned, colonnes_non_pertinentes)\n",
    "    print(f'dropping {len(cols_to_drop)} unused columns')\n",
    "    return df.drop(columns=cols_to_drop)\n",
    "\n",
    "\n",
    "df_cleaned = (\n",
    "    df_data\n",
    "    .pipe(fill_missing_values)\n",
    "    .pipe(drop_unused_columns)\n",
    ")\n",
    "\n",
    "print(f'{df_data.shape} --> {df_cleaned.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering / Preprocessing\n",
    "\n",
    "Features to create to classify texts:\n",
    "\n",
    "- Preparation of ** categories ** from `product_category_tree\n",
    "- Preparation of ** cleaned descriptions ** to enter models\n",
    "- Creation of ** topics ** (subjects) based on these cleaned descriptions (to compare with the categories of `product_category_tree`))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Preparation of categories\n",
    "\n",
    "Features to be created from `product_category_tree`:\n",
    "\n",
    "- `Categ_level_1`: categories at the root of the tree\n",
    "- `Categ_level_2`: subcategories (divisions in` Categ_level_1`)\n",
    "- `Categ_level_3`: mini-categories (divisions in` Categ_level_2`)\n",
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_brackets(node: str):\n",
    "    node = node.replace('[\"', '').replace('\"]', '')\n",
    "    return node\n",
    "\n",
    "\n",
    "def create_categ_level(df):\n",
    "    if not 'product_category_tree' in df.columns:\n",
    "        return df\n",
    "    else:\n",
    "        df_cat = (df['product_category_tree']\n",
    "                  .map(lambda x: remove_brackets(x))\n",
    "                  .str.split(' >> ', expand=True))\n",
    "        print(f'create_categ_level, shape= {df_cat.shape}')\n",
    "        depth = len(df_cat.columns)\n",
    "        df_cat.columns = [f'categ_level_{i}' for i in range(1, depth+1)]\n",
    "        return pd.concat([df, df_cat], axis=1)\n",
    "\n",
    "\n",
    "df_cleaned = (\n",
    "    df_data\n",
    "    .pipe(fill_missing_values)\n",
    "    .pipe(drop_unused_columns)\n",
    "    .pipe(create_categ_level)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_cleaned.columns:\n",
    "    if 'categ_level_' in col:\n",
    "        print(f'{col}.nunique() = {df_cleaned[col].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Categories - Level 1\n",
    "\n",
    "- We will first try to classify by level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleaned['categ_level_1'].unique().tolist())\n",
    "plot_bar_top_n(df_cleaned, 'categ_level_1', normalize=True,\n",
    "               subtitle='Level 1 Categories')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Categories - Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleaned['categ_level_2'].nunique())\n",
    "plot_bar_top_n(df_cleaned, 'categ_level_2', normalize=True,\n",
    "               subtitle='Level 2 Categories')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_cleaned[['categ_level_2', 'categ_level_1']].value_counts().head(20),\n",
    "           df_cleaned[['categ_level_2', 'categ_level_1']].value_counts(\n",
    ").cumsum().head(20).rename('cum_sum'),\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Categories tree, even if there are 62 level 2 categories, approximately 90% of the products are in the 20 largest categories of level 2.\n",
    "\n",
    "- We can also assess the level 2 classification performance, to better understand the descriptions that are best classified in each main category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Categories - Level 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleaned['categ_level_3'].nunique())\n",
    "plot_bar_top_n(df_cleaned, 'categ_level_3', normalize=True,\n",
    "               subtitle='Level 3 Categories')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 1050 data, it will be very difficult to classify between 241 different level 3 categories.\n",
    "\n",
    "- For example, distinguish between the brands of 'Coffee Mugs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Preparation (preprocite) of data descriptions\n",
    "\n",
    "We are trying to use the product 'field of products to classify them in levels of levels 1, 2, 3 ...\n",
    "\n",
    "It is necessary to prepare the descriptions before providing them to the models of machine learning:\n",
    "\n",
    "- Put everything in tiny\n",
    "- Remove the liaison words\n",
    "- Remove the punctuation\n",
    "- Remove the numbers (if necessary)\n",
    "- Transform sentences into a tokens list (in the words list)\n",
    "- Remove connecting words or that does not provide meaning (** stopwords **)\n",
    "- Lemmatizer\n",
    "- Reform sentences with the remaining words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Features to be created from the description`:\n",
    "\n",
    "Each model has different requisites:\n",
    "\n",
    "-`Sentence_bow`: Descriptions in tiny, without unnecessary words, but not lemmatizes for Bow (Bag-of-Words) and TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "-We will use these 'Bag-of-Words' for 'Baseline' Model\n",
    "- `Sentence_bow_lem`: Treatment of` Senge_bow`, Lemmatizae for Bow, TF-IDF and Word2 de\n",
    "- `sentence_dl`: cleaned descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Options for cleaning:\n",
    "\n",
    "- NLTK library\n",
    "- Library Gens.utils\n",
    "- Our own library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1.2 IMPORT from NLTK for text cleaning\n",
    "\n",
    "We use the NLTK bookstore (Natural Language Toolkit), to help cleaning\n",
    "\n",
    "We download stopwords, punctuation and lemmatization\n",
    "\n",
    "- <https://www.nltk.org/data.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "\n",
    "# stopwords\n",
    "nltk.download('stopwords')\n",
    "# punctuation\n",
    "nltk.download('punkt')\n",
    "# lemmatization\n",
    "nltk.download('wordnet')\n",
    "# Open Multilingual Wordnet Project Lemmatizations\n",
    "nltk.download('omw-1.4')\n",
    "pass\n",
    "\n",
    "# Define the Language You Want To Use To Clean The Text Field `Description`\n",
    "NTLK_LANGUAGE = 'english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 frequent words in descriptions\n",
    "\n",
    "Before cleaning of descriptions, we look at the most used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS_EN = list(set(nltk.corpus.stopwords.words('english')))\n",
    "\n",
    "\n",
    "def word_filter(list_words, stop_w=STOP_WORDS_EN):\n",
    "    return [w for w in list_words if not w in stop_w]\n",
    "\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r'[a-z]+')\n",
    "\n",
    "\n",
    "def freq_words(sentences: pd.Series,\n",
    "               token_fct=tokenizer.tokenize,\n",
    "               filter_fct=word_filter,\n",
    "               exclude=STOP_WORDS_EN,\n",
    "               include=None,\n",
    "               show_freq=True,\n",
    "               normalize=True,\n",
    "               nb=10) -> pd.DataFrame:\n",
    "\n",
    "\n",
    "    corpus = sentences.str.lower().map(lambda x: token_fct(x))\n",
    "    corpus = corpus.map(lambda x: filter_fct(x)).explode()\n",
    "\n",
    "\n",
    "    if not exclude is None:\n",
    "        corpus = [w for w in corpus if not w in exclude]\n",
    "\n",
    "    word_count = ((pd.Series(corpus)\n",
    "                  .value_counts(normalize=normalize)\n",
    "                   .to_frame('freq'))\n",
    "                  .rename_axis('word')\n",
    "                  .reset_index()\n",
    "                  )\n",
    "\n",
    "    if not include is None:\n",
    "# # Include Only The Selected Words\n",
    "# # do after value_counts for percentage frequency\n",
    "        word_count = word_count[word_count['word'].isin(include)]\n",
    "\n",
    "    nb = min(nb, len(word_count))\n",
    "    word_count = word_count.head(nb)\n",
    "\n",
    "    if show_freq == False:\n",
    "# # Take Only Top Words\n",
    "        word_count = word_count.drop('freq', axis=1).T\n",
    "    elif normalize:\n",
    "# # Return Top Words and Frequency (%)\n",
    "        word_count['freq'] *= 100\n",
    "    else:\n",
    "# # Return Top Words and Effective (Counting)\n",
    "\n",
    "        word_count = word_count.rename(columns={'freq': 'count'})\n",
    "\n",
    "# # Return is transposed to reduce the display space\n",
    "    return word_count.T\n",
    "\n",
    "\n",
    "print(\"most frequent words in 'product_name''\")\n",
    "print(freq_words(df_cleaned['product_name']))\n",
    "print('----------------------------')\n",
    "print(\"most frequent words in 'description'\")\n",
    "print(freq_words(df_cleaned['description']))\n",
    "freq_words(df_cleaned['description'], nb=20, show_freq=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the 20 most frequent words do not seem descriptive of a specific category, except the word 'Watch'\n",
    "\n",
    "- They are rather the words of advertising, dimensions or color.\n",
    "\n",
    "We check if these words are present more in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def freq_words_by_category(df: pd.DataFrame,\n",
    "                           categ_col='categ_level_1',\n",
    "                           desc_col='description',\n",
    "                           exclude=None,\n",
    "                           include=None,\n",
    "                           show_freq=True,\n",
    "                           normalize=True,\n",
    "                           nb=10) -> pd.DataFrame:\n",
    "    # Initialize an empty DataFrame to hold the results\n",
    "    df_top = pd.DataFrame()\n",
    "\n",
    "    # Retrieve unique categories from the specified category column\n",
    "    categories = df[categ_col].unique()\n",
    "\n",
    "    # Loop through each category\n",
    "    for categ in categories:\n",
    "        # Select descriptions corresponding to the current category\n",
    "        item_descriptions = df[df[categ_col] == categ][desc_col]\n",
    "\n",
    "        # Assuming 'freq_words' is a predefined function that returns a DataFrame of word frequencies\n",
    "        df_categ_words = freq_words(item_descriptions, exclude=exclude, include=include,\n",
    "                                    show_freq=show_freq, normalize=normalize, nb=nb)\n",
    "        if show_freq:\n",
    "            df_categ_words = df_categ_words.T.set_index('word').T\n",
    "        else:\n",
    "            df_categ_words = df_categ_words.T\n",
    "\n",
    "        # Add a column for the category\n",
    "        df_categ_words[categ_col] = categ\n",
    "\n",
    "        # Append the current category DataFrame to the top DataFrame\n",
    "        df_top = pd.concat([df_top, df_categ_words], ignore_index=True)\n",
    "\n",
    "    return df_top\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 visualizes like Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from WordCloud Import Wordcloud\n",
    "\n",
    "def plot_wordcloud(sentences: pd.Series, cmap='nipy_spectral', ax=None, nb=20):\n",
    "    cloud = WordCloud(stopwords=None, background_color=None,\n",
    "                      colormap=cmap)\n",
    "    topwords: pd.DataFrame = freq_words(sentences, nb=nb).T.set_index('word')\n",
    "    topword_dict = dict(zip(list(topwords.index), list(topwords['freq'])))\n",
    "# # print (topword_dict)\n",
    "    cloud.generate_from_frequencies(topword_dict)\n",
    "    if ax is None:\n",
    "        plt.figure()\n",
    "        ax = plt.gca()\n",
    "    ax.imshow(cloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "def plot_wordclouds_by_categ(df, feature='description', categ_col='categ_level_1',\n",
    "                             cmap='nipy_spectral', nb=10):\n",
    "    categories = sorted(list(df[categ_col].unique()))\n",
    "    colors = sns.color_palette(cmap, n_colors=len(categories)).as_hex()\n",
    "# # print (colors)\n",
    "# # print (categories)\n",
    "    n_cols = 4\n",
    "    n_rows = len(categories) // n_cols + (len(categories) % n_cols > 0)\n",
    "    _ = plt.figure(figsize=(n_cols*4, n_rows*3))\n",
    "    for n, category in enumerate(categories):\n",
    "        ax = plt.subplot(n_rows, n_cols, n + 1)\n",
    "        sentences = df[df[categ_col] == category][feature]\n",
    "        color = sns.color_palette(f'light:{colors[n]}', as_cmap=True)\n",
    "        plot_wordcloud(sentences, ax=ax, cmap=color, nb=nb)\n",
    "        plt.title(category, fontweight='bold')\n",
    "\n",
    "    plt.subplots_adjust(hspace=0, wspace=0.2)\n",
    "\n",
    "\n",
    "plot_wordclouds_by_categ(df_cleaned, feature='description',\n",
    "                         categ_col='categ_level_1', cmap='Set1')\n",
    "plt.suptitle('Frequent words in each level 1 category')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advertising words are present more in certain categories\n",
    "\n",
    "- They are false category indicators, as probably added if the product lacks description, or after categorization of the product.\n",
    "\n",
    "We compare with the most frequent words in 'Product_Name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words_by_category(df_cleaned, desc_col='product_name', show_freq=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordclouds_by_categ(df_cleaned, feature='product_name',\n",
    "                         categ_col='categ_level_1', cmap='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ask if the classification will work better on product_name, because it is cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = np.random.randint (0, 1050)\n",
    "seed = 18\n",
    "print(f\"[{seed}] : {df_data['product_name'][seed]}\")\n",
    "print(f\"[{seed}] : {df_data['description'][seed]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Removing \"Flipkart\" advertising (Adwords)\n",
    "\n",
    "We see that many of the descriptions contains sentences that have nothing to do with the product.\n",
    "\n",
    "For products that do not have a description or special, perhaps Flipkart have added these advertising sentences to leave the space of empty description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data['description'][795])\n",
    "print(df_data['description'][263])\n",
    "print(df_data['description'][746])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example :\n",
    "\n",
    "> '** buy ** Ecraftindia Floral Cushions Cover at RS. 404 ** at flipkart.com.Only Genuine Products.Free Shipping.Cash on Delivery! ** '\n",
    "\n",
    "These sentences make noise and reduces discrimination between products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADVERTS = [\n",
    "    'Buy', 'Only Genuine Products', '!', 'Cash On Delivery', 'Free Shipping', '30 Day Replacement Guarantee',\n",
    "    'Online', 'at Flipkart.com', 'from Flipkart.com', 'Flipkart.com', 'best prices', 'Lowest Prices',\n",
    "    'Great Discounts', 'in India Only'\n",
    "]\n",
    "\n",
    "\n",
    "def get_useless_adwords(df, adverts=ADVERTS):\n",
    "\n",
    "    df = df.copy()\n",
    "    ad_cols = []\n",
    "    for idx, ad in enumerate(adverts):\n",
    "        ad_col = f'AD_{idx}'\n",
    "        ad_cols.append(ad_col)\n",
    "        df[ad_col] = df['description'].str.lower().str.contains(ad.lower())\n",
    "\n",
    "    print(df[ad_cols].sum(axis=0))\n",
    "\n",
    "\n",
    "get_useless_adwords(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deletion of advertising sentences in descriptions\n",
    "\n",
    "Advertisements seem to be added that sentences\n",
    "\n",
    "- We delete them here as a sentence\n",
    "\n",
    "Note: an alternative will be to add the words to the 'Stopwords', but\n",
    "\n",
    "- We risk losing the same words out of the context of advertisements\n",
    "- This procedure is not called in the pre -treatment of Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "\n",
    "def remplace(sentence: str, old_str: str, new_str: str = ' ', case_sensitive=False) -> str:\n",
    "\n",
    "    if case_sensitive:\n",
    "        return sentence.replace(old_str, new_str)\n",
    "    else:\n",
    "        return re.sub(re.escape(old_str), new_str, sentence, flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "def remove_useless_adwords(sentence: str, adverts=ADVERTS):\n",
    "\n",
    "    for ad in adverts:\n",
    "        sentence = remplace(sentence, ad, ' ')\n",
    "    return sentence\n",
    "\n",
    "\n",
    "TEST_ADVERTS = df_data['description'][746]\n",
    "print(TEST_ADVERTS)\n",
    "TEST_DESCRIPTION_1 = remove_useless_adwords(TEST_ADVERTS)\n",
    "print(TEST_DESCRIPTION_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Deletion of prices in product description\n",
    "\n",
    "We can consider that the price is not a product description:\n",
    "\n",
    "- The products were obtained on various dates\n",
    "- prices depend on the seller, date, currencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_prices(sentence: str) -> str:\n",
    "\n",
    "# # \\ BRS.[0-9]*\\.[0-9]*\\ b, tested at https://regex101.com/\n",
    "    if isinstance(sentence, list):\n",
    "        sentence = (' ').join(sentence)\n",
    "# # Remove the Space Between RS.And the love, if necessary\n",
    "    sentence = remplace(sentence, 'Rs. ', 'Rs.')\n",
    "    sentence = remplace(sentence, 'at Rs.', 'Rs.')\n",
    "    sentence = remplace(sentence, 'only for Rs.', 'Rs.')\n",
    "    sentence = remplace(sentence, 'for Rs.', 'Rs.')\n",
    "    sentence = remplace(sentence, 'Price Rs.', 'Rs.')\n",
    "    sentence = remplace(sentence, 'Price:', ' ')\n",
    "# # If the Amount is decimal\n",
    "    sentence = re.sub(r'\\b[Rr]s.[0-9]*[\\.,][0-9]*\\b', ' ', sentence).strip()\n",
    "# # If the Amount is within Decimal Point\n",
    "    sentence = re.sub(r'\\b[Rr]s.[0-9]*\\b', ' ', sentence).strip()\n",
    "# # Deletion of Double Spaces Left by Deleting Words\n",
    "    sentence = re.sub('\\s+', ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "print(TEST_DESCRIPTION_1)\n",
    "TEST_DESCRIPTION_2 = remove_prices(TEST_DESCRIPTION_1)\n",
    "print(TEST_DESCRIPTION_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process test to remove advertisements and prices\n",
    "\n",
    "The above processes are improved by testing randomly on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = np.random.randint (0, 1050)\n",
    "seed = 237\n",
    "test_description = df_data['description'][seed]\n",
    "test_cleaned = remove_prices(remove_useless_adwords(test_description))\n",
    "print(f'original [{seed}] : \\n{test_description}')\n",
    "print(f'cleaned [{seed}] : \\n{test_cleaned}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Tokenization\n",
    "\n",
    "Tokenization is the process of converting a chain into tokens (in general, words and punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import string\n",
    "import re\n",
    "\n",
    "def lower_start_fct(list_words: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Convert words to lowercase and filter out words starting with '@' or 'http'\n",
    "    \"\"\"\n",
    "    lw = [w.lower() for w in list_words if (not w.startswith(\"@\"))\n",
    "          and (not w.startswith(\"http\"))]\n",
    "    return lw\n",
    "\n",
    "def remove_useless_adwords(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Placeholder for ad word removal function\n",
    "    \"\"\"\n",
    "    return text\n",
    "\n",
    "def remove_prices(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Placeholder for price removal function\n",
    "    \"\"\"\n",
    "    return text\n",
    "\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Simple word tokenization using regex\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "def tokenizer_fct(sentence: str, sans_ads=False) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize and clean text by removing punctuation, digits, and special characters\n",
    "    \"\"\"\n",
    "    if sans_ads:\n",
    "        sentence = remove_useless_adwords(sentence)\n",
    "        sentence = remove_prices(sentence)\n",
    "\n",
    "    # Clean the text\n",
    "    sentence = re.sub('\\S*@\\S*\\s?', '', sentence)  # remove emails\n",
    "    sentence = re.sub('\\s+', ' ', sentence)  # remove newline chars\n",
    "    sentence = re.sub(\"\\'\", \"\", sentence)  # remove single quotes\n",
    "\n",
    "    # Remove punctuation\n",
    "    sentence = ''.join([i for i in sentence if i not in string.punctuation])\n",
    "\n",
    "    # Remove digits\n",
    "    sentence = ''.join(i for i in sentence if not i.isdigit())\n",
    "\n",
    "    # Convert to lowercase and clean special characters\n",
    "    sentence_clean = (sentence.lower()\n",
    "                     .replace('-', ' ')\n",
    "                     .replace('+', ' ')\n",
    "                     .replace('/', ' ')\n",
    "                     .replace(',', ' ')\n",
    "                     .replace('#', ' '))\n",
    "\n",
    "    # Tokenize using simple word boundary matching\n",
    "    return simple_tokenize(sentence_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 Stop Words\n",
    "\n",
    "Stopwords in context for other words.\n",
    "We do not delete it for Deep-learning like Bert and Use\n",
    "\n",
    "For other algorithms:\n",
    "\n",
    "- We delete the stopwords of the NLTK bookstore (in the language of descriptions)\n",
    "- We analyze the most frequent words that remain\n",
    "\n",
    "- We eliminate:\n",
    "- The most frequent words not discriminatory (noise)\n",
    "- Words that are not a product discriminatory ('Key features',')\n",
    "- Forgotten words of advertising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS_EN = list(set(nltk.corpus.stopwords.words('english')))\n",
    "print(STOP_WORDS_EN[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import list\n",
    "\n",
    "\n",
    "def lower_start_fct(list_words: List[str]) -> List[str]:\n",
    "\n",
    "    lw = [w.lower() for w in list_words if (not w.startswith(\"@\"))\n",
    "# # and (not w.startswith (\"#\"))\n",
    "          and (not w.startswith(\"http\"))]\n",
    "    return lw\n",
    "\n",
    "\n",
    "def stop_word_en_filter(list_words: List[str], stop_w=STOP_WORDS_EN) -> List[str]:\n",
    "\n",
    "    filtered_w = [w for w in list_words if not w in stop_w]\n",
    "    return filtered_w\n",
    "\n",
    "\n",
    "print(stop_word_en_filter(tokenizer_fct(TEST_ADVERTS, sans_ads=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the most frequent words in the corpus\n",
    "\n",
    "We are looking for frequent non-discriminatory words.\n",
    "\n",
    "- some frequent words are useful for classification, others not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import list\n",
    "\n",
    "def get_corpus_freq(descriptions: pd.Series, nb=20, stop_w=STOP_WORDS_EN) -> List[str]:\n",
    "    word_lists = descriptions.str.lower().map(lambda x: tokenizer_fct(x))\n",
    "    corpus = word_lists.explode()\n",
    "    print(f'unique words: {corpus.nunique()}')\n",
    "    corpus_2 = pd.Series(stop_word_en_filter(corpus, stop_w=stop_w))\n",
    "    most_freq = corpus_2.value_counts()[:nb]\n",
    "\n",
    "    return most_freq\n",
    "\n",
    "\n",
    "cleaned_corpus = df_data['description'].map(\n",
    "    lambda x: remove_prices(remove_useless_adwords(x)))\n",
    "\n",
    "most_freq = get_corpus_freq(cleaned_corpus, nb=40)\n",
    "print(f'most_freq: {most_freq.index.to_list()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of a specific stopwords list for this corpus\n",
    "\n",
    "It is added specific stopwords of this corpus to the end of reducing noise words in the corpus.\n",
    "\n",
    "- The most common words\n",
    "- The least frequent words\n",
    "- Words with an IDF score (reverse Frequency Document) very low\n",
    "\n",
    "-<https://kavita-ganesan.com/tips-forcstructting-custom-top-word-lists/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_frequent = freq_words(df_cleaned['description'], nb=20)\n",
    "stop_frequent = stop_frequent.T['word'].tolist()\n",
    "for word in ['watch', 'box', 'set', 'pack', 'color', 'cm']:\n",
    "    if word in stop_frequent:\n",
    "        stop_frequent.remove(word)\n",
    "stop_frequent.extend(['details'])\n",
    "print(stop_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Sklearn.FEATURE Extraction.Text Import TF IDF QUOTRIER\n",
    "\n",
    "def get_low_idf_words(sentences: pd.Series):\n",
    "    tfv = TfidfVectorizer(use_idf=True)\n",
    "    sentences = sentences.map(lambda x: (' ').join(\n",
    "        stop_word_en_filter(tokenizer_fct(x))))\n",
    "    tfv.fit_transform(sentences)\n",
    "    df_idf = pd.DataFrame({'word': tfv.get_feature_names_out(),\n",
    "                           'idf': tfv.idf_})\n",
    "    return df_idf.sort_values(by='idf')\n",
    "\n",
    "\n",
    "idf = get_low_idf_words(df_cleaned['description'])\n",
    "idf.head(30).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_low_idf = idf.head(20)['word'].to_list()\n",
    "# We Keep the Frequent Words Indicative of Certain Categories\n",
    "# 'Watch' For Category 'Watches'\n",
    "for word in ['watch', 'box', 'set', 'pack', 'color', 'cm']:\n",
    "    if word in stop_low_idf:\n",
    "        stop_low_idf.remove(word)\n",
    "print(stop_low_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "STOP_WORDS_BASIC = ['[', ']', ',', '.', ':', '!',\n",
    "                    '?', '(', ')', '%', '&', \"'\", \"''\", \"'s\"]\n",
    "\n",
    "# frequent\n",
    "# stop_frequent = ['Key', 'GENERAL', 'Detail', 'Details',\n",
    "# 'feature', 'features', 'specification', 'specials']\n",
    "stop_id = ['brand', 'product', 'products', 'id', 'model']\n",
    "stop_pub = ['sales', 'price', 'warranty']\n",
    "stop_adjectives = ['best', 'design', 'yes']\n",
    "\n",
    "\n",
    "# Stop_words_custom = [ *stop_frequent, *stop_id, *stop_pub, *stop_adctives]\n",
    "STOP_WORDS_CUSTOM = list({*stop_frequent, *stop_low_idf})\n",
    "# Stop_words_custom = list (set ([*stop_low_idf])))))))\n",
    "print(f'stop_words_custom : {STOP_WORDS_CUSTOM}')\n",
    "\n",
    "timestr = time.strftime(\"%Y-%m-%d-%H.%M.%S\")\n",
    "pd.DataFrame(STOP_WORDS_CUSTOM).to_csv(\n",
    "    f'{IMAGE_FOLDER}/stopwords_custom_{timestr}.csv')\n",
    "\n",
    "STOP_WORDS_ALL = list(\n",
    "    set([*STOP_WORDS_EN, *STOP_WORDS_BASIC, *STOP_WORDS_CUSTOM]))\n",
    "most_freq = get_corpus_freq(cleaned_corpus, stop_w=STOP_WORDS_ALL, nb=40)\n",
    "print(f'most_frequent other words: {most_freq.index.to_list()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_w_en = list(set([*STOP_WORDS_EN, *STOP_WORDS_BASIC]))\n",
    "\n",
    "freq_words(df_data['description'].str.lower(),\n",
    "           exclude=stop_w_en,\n",
    "           include=STOP_WORDS_CUSTOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words(df_data['description'].str.lower(),\n",
    "           exclude=stop_w_en,\n",
    "           include=STOP_WORDS_CUSTOM,\n",
    "           normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of Custom Stopwords in Description\n",
    "freq_words_by_category(df_cleaned, desc_col='description',\n",
    "                       include=STOP_WORDS_CUSTOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of Custom Stopwords in Product_Name\n",
    "freq_words_by_category(df_cleaned, desc_col='product_name',\n",
    "                       include=STOP_WORDS_CUSTOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_filter_fct(list_words: List[str], stop_w=STOP_WORDS_ALL) -> List[str]:\n",
    "\n",
    "    filtered_w = [w for w in list_words if not w in stop_w]\n",
    "    return filtered_w\n",
    "\n",
    "\n",
    "print(lower_start_fct(stop_word_filter_fct(\n",
    "    tokenizer_fct(\"The movie was not good at all.\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(lower_start_fct(stop_word_filter_fct(\n",
    "    tokenizer_fct(df_data['description'][6]), stop_w=STOP_WORDS_ALL)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import list\n",
    "# from nltk.stem import wordnetlemmatizer\n",
    "\n",
    "\n",
    "def lemma_fct(list_words: List[str]) -> List[str]:\n",
    "\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lem_w = [lemmatizer.lemmatize(w) for w in list_words]\n",
    "    return lem_w\n",
    "\n",
    "\n",
    "# Lemmatizer test (bug: men -> man?)\n",
    "TEST_LEMMA = 'boys girls men and women shows lots of highs and lows qualities in inches'.split(\n",
    "    ' ')\n",
    "print(TEST_LEMMA)\n",
    "print(lemma_fct(TEST_LEMMA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6 Common preparation of treatments\n",
    "\n",
    "To analyze descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creation of feature `sentence_dl` for Deep Learning\n",
    "\n",
    "Deep Learning models like [Bert] (https://arxiv.org/abs/1810.04805) and [use] (https://arxiv.org/abs/1803.11175) treat the words in the context of their sentences\n",
    "\n",
    "- Bert: bidirectional encoder representations from transformers\n",
    "- Use: Universal Sentence encoder\n",
    "\n",
    "With Bert and Use, you should not treat the texts too much, otherwise, you lose the context (root, lemmatization) or simply modify the texts (deletion of empty words).\n",
    "\n",
    "Product descriptions already seem treated, so we try to avoid removing stopwords and lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lower_start_fct(stop_word_filter_fct(\n",
    "    tokenizer_fct(\"The movie was not good at all.\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_dl_fct(desc_text: str) -> str:\n",
    "\n",
    "    desc_text = remove_useless_adwords(desc_text)\n",
    "    desc_text = remove_prices(desc_text)\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "# # SW = stop_word_filter_fct (word_tokens)\n",
    "    lw = lower_start_fct(word_tokens)\n",
    "    transf_desc_text = ' '.join(lw)\n",
    "    return transf_desc_text\n",
    "\n",
    "\n",
    "TEST_TRANSFORM = df_data['description'][142]\n",
    "print('---Original------')\n",
    "print(TEST_TRANSFORM)\n",
    "print('---Transformed----')\n",
    "transform_dl_fct(TEST_TRANSFORM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creation of feature `sentence_bow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_bow_fct(desc_text: str) -> str:\n",
    "\n",
    "    desc_text = remove_useless_adwords(desc_text)\n",
    "    desc_text = remove_prices(desc_text)\n",
    "\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(sw)\n",
    "# # lem_w = lemma_fct (LW)\n",
    "    transf_desc_text = ' '.join(lw)\n",
    "    return transf_desc_text\n",
    "\n",
    "\n",
    "transform_bow_fct(TEST_TRANSFORM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creation of feature `sentence_bow_lem`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_bow_lem_fct(desc_text: str) -> str:\n",
    "\n",
    "    desc_text = remove_useless_adwords(desc_text)\n",
    "    desc_text = remove_prices(desc_text)\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(sw)\n",
    "    lem_w = lemma_fct(lw)\n",
    "    transf_desc_text = ' '.join(lem_w)\n",
    "    return transf_desc_text\n",
    "\n",
    "\n",
    "print(TEST_ADVERTS)\n",
    "print(transform_bow_lem_fct(TEST_ADVERTS))\n",
    "print(TEST_TRANSFORM)\n",
    "print(transform_bow_lem_fct(TEST_TRANSFORM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_bow_lem_ads_fct(desc_text: str) -> str:\n",
    "\n",
    "# # Desc_Text = Remove_useless_adwords (Desc_Text)\n",
    "# # Desc_Text = Remove_price (Desc_Text)\n",
    "    word_tokens = tokenizer_fct(desc_text)\n",
    "    sw = stop_word_filter_fct(word_tokens)\n",
    "    lw = lower_start_fct(sw)\n",
    "    lem_w = lemma_fct(lw)\n",
    "    transf_desc_text = ' '.join(lem_w)\n",
    "    return transf_desc_text\n",
    "\n",
    "\n",
    "print(TEST_ADVERTS)\n",
    "transform_bow_lem_ads_fct(TEST_ADVERTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.7 Text transformation ready for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_field(df: pd.DataFrame, text_col: str = 'description') -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['sentence_bow'] = df[text_col].apply(lambda x: transform_bow_fct(x))\n",
    "    df['sentence_bow_lem'] = df[text_col].apply(\n",
    "        lambda x: transform_bow_lem_fct(x))\n",
    "    df['product_name_bow_lem'] = df['product_name'].apply(\n",
    "        lambda x: transform_bow_lem_fct(x))\n",
    "    df['sentence_bow_lem_ads'] = df[text_col].apply(\n",
    "        lambda x: transform_bow_lem_ads_fct(x))\n",
    "    df['sentence_dl'] = df[text_col].apply(lambda x: transform_dl_fct(x))\n",
    "    print(f'transform_field [{text_col}], df.shape={df.shape}')\n",
    "    return df\n",
    "\n",
    "\n",
    "df_cleaned = df_cleaned.pipe(transform_field)\n",
    "df_cleaned.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.9 Calculation of the length of the sentences (number of words)\n",
    "\n",
    "Some models need to know the length of the sentence as a hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_length_bow(df: pd.DataFrame):\n",
    "    def tokenizer_(x): return len(nltk.tokenize.word_tokenize(x))\n",
    "    df = df.copy()\n",
    "    df['length_bow'] = df['sentence_bow'].apply(tokenizer_)\n",
    "    print(f\"max length bow : {df['length_bow'].max()}\")\n",
    "    df['length_dl'] = df['sentence_dl'].apply(tokenizer_)\n",
    "\n",
    "    print(f\"max length dl : {df['length_dl'].max()}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 pipeline for creation of features 'sentence' for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from typing import Callable\n",
    "\n",
    "def calc_length_bow(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate length of bag-of-words and direct learning sentences using simple word splitting.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing 'sentence_bow' and 'sentence_dl' columns\n",
    "    Returns:\n",
    "        DataFrame with added 'length_bow' and 'length_dl' columns\n",
    "    \"\"\"\n",
    "    def tokenizer_(x: str) -> int:\n",
    "        if pd.isna(x):\n",
    "            return 0\n",
    "        # Use simple whitespace tokenization instead of NLTK's word_tokenize\n",
    "        return len(str(x).split())\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Calculate lengths and add logging\n",
    "    df['length_bow'] = df['sentence_bow'].apply(tokenizer_)\n",
    "    print(f\"max length bow: {df['length_bow'].max()}\")\n",
    "    print(f\"mean length bow: {df['length_bow'].mean():.2f}\")\n",
    "\n",
    "    df['length_dl'] = df['sentence_dl'].apply(tokenizer_)\n",
    "    print(f\"max length dl: {df['length_dl'].max()}\")\n",
    "    print(f\"mean length dl: {df['length_dl'].mean():.2f}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def safe_pipe(df: pd.DataFrame, func: Callable, func_name: str, *args, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Safely apply a pipeline function with error handling and logging.\n",
    "\n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        func: Function to apply\n",
    "        func_name: Name of the function for logging\n",
    "        *args, **kwargs: Additional arguments for the function\n",
    "    Returns:\n",
    "        Processed DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = func(df, *args, **kwargs)\n",
    "        print(f\"Applied {func_name}, shape={result.shape}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {func_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_data(raw_data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main data processing pipeline.\n",
    "\n",
    "    Args:\n",
    "        raw_data_path: Path to the raw CSV file\n",
    "    Returns:\n",
    "        Processed DataFrame\n",
    "    \"\"\"\n",
    "    # Create transform_field step with 'description' parameter\n",
    "    transform_desc = lambda df: transform_field(df, 'description')\n",
    "\n",
    "    pipeline = [\n",
    "        (fill_missing_values, \"fill_missing_values\"),\n",
    "        (drop_unused_columns, \"drop_unused_columns\"),\n",
    "        (create_categ_level, \"create_categ_level\"),\n",
    "        (transform_desc, \"transform_field_description\"),\n",
    "        (calc_length_bow, \"calc_length_bow\")\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(raw_data_path, sep=',', encoding='UTF-8')\n",
    "\n",
    "        for func, name in pipeline:\n",
    "            df = safe_pipe(df, func, name)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Usage\n",
    "df_cleaned = process_data(RAW_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Verification of more frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('---- bag-of-words ------')\n",
    "get_corpus_freq(df_cleaned['sentence_bow'])\n",
    "print('---- bag-of-words with lemmatization ------')\n",
    "get_corpus_freq(df_cleaned['sentence_bow_lem'])\n",
    "print('---- sentence for deep learning ------')\n",
    "get_corpus_freq(df_cleaned['sentence_dl'])\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Visualize as Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordclouds_by_categ(\n",
    "    df_cleaned, feature='sentence_bow_lem', categ_col='categ_level_1')\n",
    "plt.suptitle('Frequent words in each level 1 category after cleaning')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 saves prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_make_dir(OUT_FOLDER)\n",
    "df_cleaned.to_csv(f'{OUT_FOLDER}/data_cleaned.csv',\n",
    "                  encoding='UTF-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Cleaning global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_data\n",
    "del df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Common functions for NLP models\n",
    "\n",
    "- Data reading\n",
    "- Model scoring\n",
    "- Visualization of performance metrics\n",
    "- Visualization of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_T = pd.read_csv(f'{OUT_FOLDER}/data_cleaned.csv')\n",
    "print(data_T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Variables to compare the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Sklearn.Preprocessing import labelencoder\n",
    "cat_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# List of Level 1 categories\n",
    "list_categories = sorted(list(set(data_T['categ_level_1'])))\n",
    "\n",
    "l_cat_num = cat_encoder.fit_transform(list_categories)\n",
    "\n",
    "# Category Map\n",
    "cat_map = dict(zip(l_cat_num, list_categories))\n",
    "print(\"categories : \", cat_map)\n",
    "cat_encoder.inverse_transform(l_cat_num)\n",
    "\n",
    "\n",
    "y_cat_num = cat_encoder.transform(data_T['categ_level_1'])\n",
    "# True Category\n",
    "y_cat_txt = data_T['categ_level_1'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.inverse_transform([1, 0, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Modeling and evaluation of models:\n",
    "\n",
    "- ** PRESTRATION ** Data (choice of description, lemmatization, stopwords, ...) - see above.We use 'feature' to choose the column of preterity data.\n",
    "\n",
    "-** Extraction of features ** (by bag-of-words, TF-IDF, Word2vec etc)\n",
    "\n",
    "- ** Reduction of dimensions ** (by t-t, PCA, NMF, truncatedsvd ...)\n",
    "\n",
    "- ** Classification (clustering) not supervised on reduced dimensions ** (gridsearch of hyperparammeters, without or with choice of clusters)\n",
    "\n",
    "- by Kmeans (defect)\n",
    "- by LDA (Topic Modeling)\n",
    "- by NMF (not used here)\n",
    "\n",
    "- ** Performance evaluation **:\n",
    "\n",
    "- Score distortion (inertia or sum of square errors of each cluster)\n",
    "- Davies_bouldin score (clusters separation measure)\n",
    "- Silhouette score of clusters (+ visualization of the silhouettes of each cluster)\n",
    "-Stability score of clusters (on sub-samples-> Type deviation)\n",
    "\n",
    "- ** Evaluation of correspondence with the defined categories **\n",
    "- Ari Score\n",
    "- Percentage of badly classified items\n",
    "- Visualization to compare clusters on the main reduced dimensions (tsne or PCA for example)\n",
    "- Sankey visualization of correcting between clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Scoring: performance of a model\n",
    "\n",
    "It is complicated to set up a preprocessing pipeline, extraction feature, reduction dimension, classification, scoring, because the bookstores sometimes return a sparse matrix, sometimes not.\n",
    "\n",
    "Below, my generic procedure for score a model\n",
    "\n",
    "- We vary the parameter K to assess the number of clusters for better performance\n",
    "\n",
    "You can easily add a param_grid and make a gridsearch on all combinations of the parameters:\n",
    "\n",
    "`` python\n",
    "# generate parameter grid for classif\n",
    "param_sets = parametergrid (param_grid)\n",
    "\n",
    "for params in param_sets:\n",
    "# print (f'Score model for params = {params} ')\n",
    "# Set Model Parameters (usually just n_conlusters for classify)\n",
    "Model.set_params (** params)\n",
    "Model.Fit ...\n",
    "`` `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import manifold, cluster, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def get_classname(estimator):\n",
    "    return estimator.__class__.__name__\n",
    "\n",
    "def score_model(df: pd.DataFrame,\n",
    "                feature='sentence_bow_lem',\n",
    "                labels_true=None,\n",
    "                document_preprocessor=None,\n",
    "                feature_extractor=TfidfVectorizer(\n",
    "                    stop_words='english', min_df=3),\n",
    "                dimension_reducer=manifold.TSNE(n_components=2, perplexity=30, n_iter=2000,\n",
    "                                              init='pca', learning_rate=200, random_state=42),\n",
    "                kmin=7, kmax=7):\n",
    "\n",
    "    # Preprocess\n",
    "    if document_preprocessor is None:\n",
    "        documents = df[feature]\n",
    "    else:\n",
    "        documents = document_preprocessor.fit_transform(df[feature])\n",
    "\n",
    "    # Extract features\n",
    "    start_fit_extract = time.time()\n",
    "    features = feature_extractor.fit_transform(documents)\n",
    "    fit_time_extract = round(time.time()-start_fit_extract, 2)\n",
    "    print(\n",
    "        f'Extract_features ({get_classname(feature_extractor)}), fit time = {fit_time_extract} s')\n",
    "    print(f'type(features) = {type(features)}')\n",
    "\n",
    "    if isinstance(features, scipy.sparse.csr_matrix):\n",
    "        print('converting features from sparse to dense array')\n",
    "        features = features.toarray()\n",
    "\n",
    "    print(f'features.shape ={features.shape}')\n",
    "\n",
    "    # Reduce Dimensions\n",
    "    start_fit_reduce = time.time()\n",
    "    reduced_dimensions = dimension_reducer.fit_transform(features)\n",
    "    fit_time_reduce = round(time.time()-start_fit_reduce, 2)\n",
    "    print(\n",
    "        f'Reduced dimensions [{reduced_dimensions.shape}] ({get_classname(dimension_reducer)}), fit time = {fit_time_reduce} s')\n",
    "\n",
    "    scores_list = []\n",
    "\n",
    "    for k in range(kmin, kmax+1):\n",
    "        clusterer = cluster.KMeans(n_clusters=k)\n",
    "\n",
    "        start_fit_clf = time.time()\n",
    "        clusterer.fit(reduced_dimensions)\n",
    "        fit_time_clf = round(time.time()-start_fit_clf, 2)\n",
    "\n",
    "        labels_pred = clusterer.labels_\n",
    "        cluster_sizes = (pd.Series(labels_pred).value_counts(\n",
    "            normalize=True).values*100).astype(int)\n",
    "\n",
    "        min_cluster_pct = pd.Series(\n",
    "            labels_pred).value_counts(normalize=True).min()\n",
    "\n",
    "        res = {\n",
    "            'k': k,\n",
    "            'min_cluster_pct': round(min_cluster_pct*100, 2),\n",
    "            'distortion_score': round(getattr(clusterer, 'inertia_', 0), 0),\n",
    "            'davies_bouldin': round(metrics.davies_bouldin_score(reduced_dimensions, labels_pred), 2),\n",
    "            'calinski_harabasz': round(metrics.calinski_harabasz_score(reduced_dimensions, labels_pred), 2),\n",
    "            'silhouette_score': round(metrics.silhouette_score(reduced_dimensions, labels_pred), 3),\n",
    "            'fit_time_cluster': fit_time_clf\n",
    "        }\n",
    "\n",
    "        ARI = ''\n",
    "        if labels_true is not None:\n",
    "            res['ARI'] = round(metrics.adjusted_rand_score(\n",
    "                labels_true, labels_pred), 3)\n",
    "            ARI = f\" ARI = {res['ARI']},\"\n",
    "\n",
    "        print(\n",
    "            f\"k={k}, fit: {fit_time_clf} s, silhouette= {res['silhouette_score']:.3},{ARI} cluster sizes (%) = {cluster_sizes}\")\n",
    "\n",
    "        scores_list.append(res)\n",
    "\n",
    "    df_scores = pd.DataFrame(scores_list)\n",
    "\n",
    "    df_scores['feature_extraction'] = get_classname(feature_extractor)\n",
    "    df_scores['dimension_reduction'] = get_classname(dimension_reducer)\n",
    "    df_scores['classifier'] = get_classname(clusterer)\n",
    "    df_scores['k'] = df_scores['k'].astype(int)\n",
    "\n",
    "    if kmin == kmax:\n",
    "        return df_scores, labels_pred, reduced_dimensions, k\n",
    "    else:\n",
    "        best_score_idx = df_scores['silhouette_score'].idxmax()\n",
    "        best_k = df_scores.loc[best_score_idx, 'k']\n",
    "        clusterer = cluster.KMeans(n_clusters=best_k)\n",
    "        clusterer.fit(reduced_dimensions)\n",
    "        labels_pred = clusterer.labels_\n",
    "        return df_scores, labels_pred, reduced_dimensions, best_k\n",
    "\n",
    "# Usage\n",
    "scores, labels, red_dim, best_k = score_model(data_T, 'sentence_bow_lem',\n",
    "                                            labels_true=data_T['categ_level_1'],\n",
    "                                            kmin=4, kmax=12)\n",
    "print(f'K for best silhouette: {best_k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Visualizations of performance metrics\n",
    "\n",
    "We are inspired by Yellowbrick Kelbowvisualizer to generalize to other metrics (Stability for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def angle_between_vectors(v1, v2):\n",
    "\n",
    "    ang1 = np.arctan2(*v1[::-1])\n",
    "    ang2 = np.arctan2(*v2[::-1])\n",
    "    ang = np.rad2deg(abs(ang1 - ang2) % (2 * np.pi))\n",
    "    if ang > 180:\n",
    "        ang = ang-180\n",
    "    return ang\n",
    "\n",
    "\n",
    "print([\n",
    "    angle_between_vectors([1, 1], [0, 0]),\n",
    "    angle_between_vectors([1, 1], [0, 1]),\n",
    "    angle_between_vectors([1, 1], [-1, 0]),\n",
    "    angle_between_vectors([1, 1], [-1, -1]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_elbow(df: pd.DataFrame, x_col='k', y_col='distortion_score'):\n",
    "\n",
    "    df = df.copy().reset_index()\n",
    "# # Standardise SCALE TO Maximise Between Stops\n",
    "\n",
    "    x_scale = df[x_col].max()-df[x_col].min()\n",
    "    y_min = df[y_col].min()\n",
    "    y_mult = (df[y_col].max()-y_min)/x_scale\n",
    "    df['y_scaled'] = (df[y_col]-y_min)/y_mult\n",
    "\n",
    "# # Calculte angles Between Conseiscive Points\n",
    "    n = len(df)\n",
    "    for i in range(0, n-2):\n",
    "        point1 = df.loc[i, [x_col, 'y_scaled']]\n",
    "        point2 = df.loc[i+1, [x_col, 'y_scaled']]\n",
    "        point3 = df.loc[i+2, [x_col, 'y_scaled']]\n",
    "        vec1 = point2-point1\n",
    "        vec2 = point3-point2\n",
    "        df.loc[i+1, 'angle'] = angle_between_vectors(vec1, vec2)\n",
    "# # Elbow is at Highest Difference in Angle (Assume Monotonic Function)\n",
    "    row = df['angle'].idxmax()\n",
    "    x_elbow = df.loc[row, x_col]\n",
    "    y_score = df.loc[row, y_col]\n",
    "    return x_elbow, y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_best, distortion_score = find_elbow(scores, y_col='distortion_score')\n",
    "print(f'Best score for k = {k_best}')\n",
    "# check labels for best score have been returned\n",
    "print(pd.Series(labels).nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Plot Elbow Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_second_ax(df, x_col, y2_col, ax, color='grey'):\n",
    "\n",
    "    if y2_col in df.columns:\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(df[x_col], df[y2_col], label=y2_col,\n",
    "                 c=color, marker='o', linestyle='--', alpha=0.75)\n",
    "        ax2.tick_params(axis='y', colors=color)\n",
    "        ax2.set_ylabel(y2_col, color=color)\n",
    "\n",
    "\n",
    "def plot_elbow(df, x_col, y_col, ax):\n",
    "\n",
    "    elbow_k, elbow_score = find_elbow(df, x_col, y_col)\n",
    "    elbow_label = f'elbow at ${x_col}={elbow_k}$'\n",
    "    ax.axvline(elbow_k, c='k', linestyle=\"--\", label=elbow_label)\n",
    "    ax.legend(frameon=True)\n",
    "\n",
    "\n",
    "def plot_vline(df, x_col, y_col, ax, line_at='max'):\n",
    "\n",
    "    if line_at == 'max':\n",
    "        row_idx = df[y_col].argmax()\n",
    "    elif line_at == 'min':\n",
    "        row_idx = df[y_col].argmin()\n",
    "    line_x = df.loc[row_idx, x_col]\n",
    "    line_label = f'{line_at} at ${x_col}={line_x}$'\n",
    "    ax.axvline(line_x, c='k', linestyle=\"--\", label=line_label)\n",
    "    ax.legend(frameon=True)\n",
    "\n",
    "\n",
    "def plot_elbow_visualiser(df: pd.DataFrame, x_col='k', score_col='distortion_score',\n",
    "                          show_elbow=False, time_col=None, ax1=None):\n",
    "\n",
    "    colors = sns.color_palette(\"tab20\").as_hex()\n",
    "    if ax1 is None:\n",
    "        _, ax1 = plt.subplots()\n",
    "# # scores\n",
    "    ax1.plot(df[x_col], df[score_col],\n",
    "             marker=\"D\", c=colors[0], linestyle=\"-\")\n",
    "    ax1.grid(False)\n",
    "    ax1.tick_params(axis='y', colors=colors[0])\n",
    "    ax1.set_xlabel(x_col)\n",
    "    ax1.set_ylabel(score_col, c=colors[0])\n",
    "# # Fit Times\n",
    "    if not time_col is None:\n",
    "        plot_second_ax(df, x_col, y2_col=time_col, ax=ax1)\n",
    "    if show_elbow:\n",
    "        plot_elbow(df, x_col, score_col, ax1)\n",
    "\n",
    "\n",
    "# Visualize Test\n",
    "plot_elbow_visualiser(scores, score_col='distortion_score', show_elbow=True)\n",
    "plt.suptitle(f'Kmeans Distortion Score')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Plot Summary Metrics\n",
    "\n",
    "- Visualize several metrics com Subpups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(list(scores.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_metrics(df_scores, x_col='k'):\n",
    "\n",
    "    if 'clf' in df_scores.columns and 'preprocessor' in df_scores.columns:\n",
    "        print(\n",
    "            f\"plotting metrics (clf: {df_scores['clf'][0]}, preprocessor: {df_scores['preprocessor'][0]})\")\n",
    "\n",
    "    METRICS = ['distortion_score', 'calinski_harabasz',\n",
    "               'davies_bouldin', 'silhouette_score', 'stability_score']\n",
    "    metrics = cols_in_df(df_scores, METRICS)\n",
    "    n_plots = len(metrics)\n",
    "    n_cols = min(n_plots, 2)\n",
    "# # n_rows = int (ceil (n_plots/n_cols))))\n",
    "    n_rows = n_plots // n_cols + (n_plots % n_cols > 0)\n",
    "    _ = plt.figure(figsize=(n_cols*5, n_rows*3))\n",
    "    for n, metric in enumerate(metrics):\n",
    "        ax = plt.subplot(n_rows, n_cols, n + 1)\n",
    "        if metric == 'distortion_score':\n",
    "            plot_elbow_visualiser(\n",
    "                df_scores, x_col, metric, ax1=ax, time_col='fit time (s)', show_elbow=True)\n",
    "            plt.title(metric)\n",
    "        if metric == 'calinski_harabasz':\n",
    "            plot_elbow_visualiser(df_scores, x_col, metric, ax1=ax)\n",
    "            plot_vline(df_scores, x_col, metric, ax=ax, line_at='max')\n",
    "            plt.title(f'{metric} score (max={df_scores[metric].max():.0f})')\n",
    "        if metric == 'davies_bouldin':\n",
    "            plot_elbow_visualiser(df_scores, x_col, metric, ax1=ax)\n",
    "            plot_vline(df_scores, x_col, metric, ax=ax, line_at='min')\n",
    "            plt.title(f'{metric} score (min={df_scores[metric].min():.2f})')\n",
    "        if metric == 'silhouette_score':\n",
    "            titre = f'{metric} (max={df_scores[metric].max():.2f})'\n",
    "            if 'silhouette_score_std' in df_scores.columns:\n",
    "                if 'silhouette_sample_sizes' in df_scores.columns:\n",
    "                    sample_size = int(\n",
    "                        df_scores['silhouette_sample_sizes'].max())\n",
    "                    titre += f' 10 samples de {sample_size} pts'\n",
    "                plt.errorbar(data=df_scores, x=x_col, y='silhouette_score',\n",
    "                             yerr='silhouette_score_std')\n",
    "            plot_elbow_visualiser(df_scores, x_col, metric,\n",
    "                                  ax1=ax, time_col='silhouette_time')\n",
    "            plot_vline(df_scores, x_col, metric, ax=ax, line_at='max')\n",
    "            plt.title(titre)\n",
    "        if metric == 'stability_score':\n",
    "            titre = f'{metric} (max={df_scores[metric].max():.2f})'\n",
    "            if 'stability_score_std' in df_scores.columns:\n",
    "                if 'stability_sample_sizes' in df_scores.columns:\n",
    "                    sample_size = int(\n",
    "                        df_scores['stability_sample_sizes'].max())\n",
    "                    titre += f' 10 samples de {sample_size} pts'\n",
    "                plt.errorbar(data=df_scores, x=x_col, y='stability_score',\n",
    "                             yerr='stability_score_std')\n",
    "            plot_elbow_visualiser(df_scores, x_col, metric,\n",
    "                                  ax1=ax, time_col='stability_time')\n",
    "            plot_vline(df_scores, x_col, metric, ax=ax, line_at='max')\n",
    "            plt.title(titre)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "    plt.suptitle(\n",
    "        f\"Plot metrics (feature extraction : {df_scores['feature_extraction'][0]};   dimension reduction {df_scores['dimension_reduction'][0]})\")\n",
    "\n",
    "\n",
    "# test\n",
    "plot_metrics(scores)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Visualization of clusters on the 'reduced dimensions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores, labels, red_dim = score_model (data_t)\n",
    "print(pd.Series(labels).nunique())\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "sns.scatterplot(\n",
    "    ax=ax1, x=red_dim[:, 0], y=red_dim[:, 1], hue=pd.Series(y_cat_txt, dtype=str))\n",
    "sns.scatterplot(\n",
    "    ax=ax2, x=red_dim[:, 0], y=red_dim[:, 1], hue=pd.Series(labels, dtype=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Visualization of clusters on the 'reduced dimensions'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Calculation of better correspondence between the category and the \"cluster label\"\n",
    "\n",
    "The labels cluster attributed by modeling (eg Kmeans) are random, that makes comparisons between the attributed category and the label are difflicil to compare: they will not have the same colors\n",
    "\n",
    "We try to match the Labels cluster in the same order as Categ_true, to facilitate:\n",
    "\n",
    "- The confusion matrix between the category and cluster\n",
    "- The comparison of clusters on tsne/pca studs (closer colors)\n",
    "- Sankey diagrams to show relationships\n",
    "\n",
    "References\n",
    "\n",
    "- https://sparse-plex.readthedocs.io/en/latest/book/Clustering/Comparing_Clusterings.html\n",
    "-https://python.plainengish.io/hungarian-algorithm-introduction-python-implementation-93e7c0890e15\n",
    "- https://docs.ssipy.org/doc/scipy/reference/generated/Scipy.optimize.linear_sum_assignment.html\n",
    "\n",
    "As an alternative to Kmeans (algorithm published October 2021):\n",
    "\n",
    "- https://www.researchgate.net/publication/353696146_the_utility_of_clusters_and_a_hungarian_clustering_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the confusion matrix\n",
    "\n",
    "When the number of classes predicted are identical to the number of \"real\" classes, the confusion matrix is ​​better calculated by the Hungarian algorithm:\n",
    "\n",
    "- associate each real class with the cluster closest in the diagonal, so as to maximize the correspondence between true classes and predicted classes (the ARI score is calculated based on this relationship 1 to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix - Approach 1: Maximize the correspondence between real class and cluster predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def squarify_df(df: pd.DataFrame, val=0):\n",
    "\n",
    "    rows, cols = df.shape\n",
    "\n",
    "    if rows > cols:\n",
    "        # Append columns to end of DataFrame\n",
    "        pad_cols = rows - cols\n",
    "        pad_df = pd.DataFrame(\n",
    "            np.zeros((rows, pad_cols), dtype=int),\n",
    "            index=df.index\n",
    "        )\n",
    "        pad_df.columns = [f'pad_{i}' for i in range(pad_cols)]\n",
    "        df = pd.concat([df, pad_df], axis=1)\n",
    "\n",
    "    elif cols > rows:\n",
    "        # Append rows to end of DataFrame\n",
    "        pad_rows = cols - rows\n",
    "        pad_df = pd.DataFrame(\n",
    "            data=np.zeros((cols-rows, cols), dtype=int),\n",
    "            columns=df.columns,\n",
    "            index=[f'pad_{i}' for i in range(pad_rows)]\n",
    "        )\n",
    "        df = pd.concat([df, pad_df], axis=0)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Test the function\n",
    "df_test_rows = pd.DataFrame([[1, 0, 6], [0, 3, 3]])\n",
    "df_test_cols = df_test_rows.T\n",
    "\n",
    "print(\"Original shape:\", df_test_rows.shape)\n",
    "print(\"\\nSquarified rows:\")\n",
    "print(squarify_df(df_test_rows))\n",
    "print(\"\\nSquarified columns:\")\n",
    "print(squarify_df(df_test_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_crosstab(y_true, y_pred, normalize=False) -> pd.DataFrame:\n",
    "\n",
    "    if not isinstance(y_true, pd.Series):\n",
    "        y_true = pd.Series(np.array(y_true), name='y_true')\n",
    "\n",
    "    if not isinstance(y_pred, pd.Series):\n",
    "        y_pred = pd.Series(np.array(y_pred), name='y_pred')\n",
    "\n",
    "    if isinstance(y_pred, pd.Series):\n",
    "# # If Both Are Series, pd.crosstab uses their clues to make crosstab\n",
    "        y_pred = y_pred.copy()\n",
    "        y_pred.index = y_true.index\n",
    "\n",
    "# # Create the Confusion Matrix Between the Category and Cluster Label\n",
    "    return pd.crosstab(y_true, y_pred, normalize=normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix_labels(categ_true: pd.Series, clust_labels: pd.Series, normalize=False):\n",
    "\n",
    "    df_matr = confusion_matrix_crosstab(\n",
    "        categ_true, clust_labels, normalize=normalize)\n",
    "\n",
    "# # Algorithm Needs A Square Matrix\n",
    "    n_categ = df_matr.shape[0]\n",
    "    n_labels = df_matr.shape[1]\n",
    "\n",
    "    square_matr = squarify_df(df_matr).fillna(0)\n",
    "\n",
    "# # Find Order of Columns and Lines to Maximize Values ​​in the Diagonal\n",
    "    rows, cols = scipy.optimize.linear_sum_assignment(\n",
    "        square_matr.values, maximize=True)\n",
    "\n",
    "# # Remove Empty Lines/Columns after Optimizing the Diagonal\n",
    "    if len(rows) > n_categ:\n",
    "        rows = [idx for idx in rows if idx < n_categ]\n",
    "    if len(cols) > n_labels:\n",
    "        cols = [idx for idx in cols if idx < n_labels]\n",
    "# # print (f'len (rows): {len (rows)} ')\n",
    "# # print (f'len (passes): {len (pass)} ')\n",
    "\n",
    "    df_opt = df_matr.iloc[:, cols]\n",
    "\n",
    "    if normalize == False:\n",
    "        df_opt = df_opt.round(0).astype(int)\n",
    "\n",
    "    return df_opt\n",
    "\n",
    "\n",
    "# test\n",
    "y_true = ['a', 'a', 'a', 'a', 'a', 'b', 'b',\n",
    "          'b', 'b', 'b', 'c', 'c', 'c', 'c', 'c']\n",
    "y_pred = [0, 3, 4, 1, 0, 4, 3, 4, 2, 4, 2, 1, 3, 1, 3]\n",
    "\n",
    "conf_matrix_labels(y_true, y_pred).style.background_gradient(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display of the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Serie = list or np.ndarray or pd.Series\n",
    "\n",
    "\n",
    "def plot_confusion_matrix_heatmap(y_true: Serie, y_pred: Serie, cmap='Blues', figsize=(6, 4)):\n",
    "\n",
    "    df_cm = conf_matrix_labels(y_true, y_pred)\n",
    "    plt.figure(figsize=figsize),\n",
    "    sns.heatmap(df_cm, annot=True, cmap=cmap, fmt='.0f')\n",
    "\n",
    "\n",
    "# test\n",
    "# test\n",
    "y_true = ['a', 'a', 'a', 'a', 'a', 'b', 'b',\n",
    "          'b', 'b', 'b', 'c', 'c', 'c', 'c', 'c']\n",
    "y_pred = [0, 3, 4, 1, 0, 4, 3, 4, 2, 4, 2, 1, 3, 1, 3]\n",
    "plot_confusion_matrix_heatmap(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix - Approach 2: Associated each cluster predicted to one of the real classes\n",
    "\n",
    "When the number of classes predicted is not identical to the number of \"real\" classes, we can present the confusion matrix between labels True and predicted labels in 2 ways:\n",
    "\n",
    "- Combine each real cluster classes predicted the closest in the diagonal (approach 1 - \"Hungarian Clustering algorithm\").The other clusters less close to the real classes are considered to be \"new\" classes - they are unknown for the calculation of the confusion matrix.\n",
    "\n",
    "- associate each class predicted to the real closest class (approach 2)\n",
    "- Several predicted classes may be associated with a real class (we treat them as a single cluster for the calculation of the confusion matrix),\n",
    "- He may have real classes without any associated cluster.\n",
    "\n",
    "Below, the functions for approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conf_mat_transform(y_true: Serie, y_pred: Serie) -> pd.Series:\n",
    "\n",
    "    conf_mat = confusion_matrix_crosstab(y_true, y_pred, False).values\n",
    "    y_predi = preprocessing.LabelEncoder().fit_transform(y_pred)\n",
    "    corresp = np.argmax(conf_mat, axis=0).astype(int)\n",
    "    print(\"Cluster matching:\", corresp)\n",
    "\n",
    "    labels = pd.Series(y_true, name=\"y_true\").to_frame()\n",
    "    labels['y_pred'] = y_predi\n",
    "    y_pred_transform = labels['y_pred'].apply(lambda x: corresp[x])\n",
    "\n",
    "    if isinstance(y_pred, pd.Series):\n",
    "        y_pred_transform = pd.Series(\n",
    "            y_pred_transform, name=f'{y_pred.name} (transformed)')\n",
    "    return y_pred_transform\n",
    "\n",
    "\n",
    "\n",
    "new_labels = conf_mat_transform(y_true, y_pred)\n",
    "pd.DataFrame({'new_label': new_labels, 'y_pred': y_pred}\n",
    "             ).value_counts().sort_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display of the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_classification_report(y_true: Serie, y_pred: Serie, target_names=None):\n",
    "\n",
    "    y_true_unique = sorted(list(set(y_true)))\n",
    "    if target_names is None:\n",
    "        target_names = [str(label) for label in y_true_unique]\n",
    "\n",
    "    is_str_y_true = np.sum([isinstance(label, str)\n",
    "                           for label in y_true_unique]) > 0\n",
    "    is_str_y_pred = np.sum([isinstance(label, str)\n",
    "                           for label in set(y_pred)]) > 0\n",
    "    mixed_types = (is_str_y_true and not is_str_y_pred) or (\n",
    "        is_str_y_pred and not is_str_y_pred)\n",
    "    if mixed_types:\n",
    "\n",
    "        y_encoder = preprocessing.LabelEncoder()\n",
    "        y_true_num = y_encoder.fit_transform(y_true)\n",
    "        print(metrics.classification_report(y_true_num, y_pred,\n",
    "              target_names=target_names, zero_division=0))\n",
    "    else:\n",
    "        print(metrics.classification_report(y_true, y_pred,\n",
    "              target_names=target_names, zero_division=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display of classification metrics\n",
    "\n",
    "To compare the models, we standardize the presentation of metrics\n",
    "\n",
    "- Ari score\n",
    "- Precision and Recall (Report classification)\n",
    "- Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ARI(labels_true, labels_pred):\n",
    "\n",
    "    return np.round(metrics.adjusted_rand_score(labels_true, labels_pred), 4)\n",
    "\n",
    "\n",
    "def plot_classification_metrics(y_tru_, y_predi_, cmap='Blues', combine=False, target_names=None):\n",
    "\n",
    "    y_true_ = y_tru_.copy()\n",
    "    y_true_unique = list(set(y_true_))\n",
    "    y_pred_unique = list(set(y_predi_))\n",
    "    if (len(y_true_unique) < len(y_pred_unique)) or combine:\n",
    "\n",
    "        print(\n",
    "            f'combining clusters {len(y_true_unique)} <  {len(y_pred_unique)}')\n",
    "        print(y_pred_unique)\n",
    "        y_pred1_ = conf_mat_transform(y_true_, y_predi_)\n",
    "    else:\n",
    "\n",
    "        y_pred1_ = conf_mat_transform(y_true_, y_predi_)\n",
    "    ari = calc_ARI(y_true_, y_pred1_)\n",
    "    print(f'ARI = {ari:.3f}')\n",
    "    print_classification_report(y_true_, y_pred1_, target_names=target_names)\n",
    "    plot_confusion_matrix_heatmap(y_true_, y_pred1_, cmap=cmap)\n",
    "    return ari\n",
    "\n",
    "\n",
    "# test\n",
    "y_true2 = pd.Series(y_true, name='categ_level_1')\n",
    "y_pred2 = pd.Series(y_pred, name='labels_LDA')\n",
    "ari = plot_classification_metrics(y_true2, y_pred2, cmap='Greens')\n",
    "plt.suptitle('test confusion matrix')\n",
    "plt.title(f'ARI = {ari:.3f}')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "y_true1 = ['a', 'a', 'a', 'b', 'b', 'b', 'c',\n",
    "           'c', 'c', 'd', 'd', 'd', 'e', 'e', 'e']\n",
    "y_pred1 = [0, 3, 3, 1, 0, 1, 3, 2, 2, 2, 2, 1, 3, 1, 3]\n",
    "print(len(y_true1), len(y_pred1))\n",
    "plot_classification_metrics(y_true1, y_pred1, cmap='Blues', combine=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of matrix confusion as a Sankey diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# Import Plotly.io as Pio\n",
    "\n",
    "\n",
    "def add_alpha(color, alpha):\n",
    "    (r, g, b) = color\n",
    "    return f'rgba({r},{g},{b},{alpha})'\n",
    "\n",
    "\n",
    "def plot_sankey_confusion_diagram(source: pd.Series, target: pd.Series = None,\n",
    "                                  titre='Sankey confusion diagram',\n",
    "                                  descriptors=['categories', 'clusters'],\n",
    "                                  figsize=(2, 1), font_size=14,\n",
    "                                  to_image=True,\n",
    "                                  palette='nipy_spectral', alpha=0.5):\n",
    "\n",
    "# # We can provides Two Series (y_true, y_pred),\n",
    "    if isinstance(source, pd.DataFrame) or len(source) < 10:\n",
    "        ct = source.copy()\n",
    "        if isinstance(ct, np.ndarray):\n",
    "            print('ct is numpy array')\n",
    "            ct = pd.DataFrame(ct)\n",
    "            ct = ct.rename_axis(descriptors[0], axis=0)\n",
    "            ct = ct.rename_axis(descriptors[1], axis=1)\n",
    "    else:\n",
    "        if isinstance(source, np.ndarray):\n",
    "            source = pd.Series(source, name=descriptors[0])\n",
    "        if isinstance(target, np.ndarray):\n",
    "            target = pd.Series(target, name=descriptors[1])\n",
    "        ct = pd.crosstab(source, target)\n",
    "\n",
    "    source_col = ct.index.name if ct.index.name else descriptors[0]\n",
    "    target_col = ct.T.index.name if ct.T.index.name else descriptors[1]\n",
    "    print(source_col, target_col)\n",
    "\n",
    "    ct.index = ct.index.astype(str)\n",
    "    ct.columns = ct.columns.astype(str)\n",
    "\n",
    "# # Replace Source and Target Labels with Unique Node_ids\n",
    "    node_labels = list(ct.index)+list(ct.columns)\n",
    "    node_ids = range(len(node_labels))\n",
    "    label_map = dict(zip(node_ids, node_labels))\n",
    "\n",
    "# # Create Colors for Unique Nodes (Nodes with Same Name Have Same Color)\n",
    "# # uniq_nodes = sorted (list (set (node_labels))))))\n",
    "    uniq_nodes = []\n",
    "    for node in node_labels:\n",
    "        if not node in uniq_nodes:\n",
    "            uniq_nodes.append(node)\n",
    "    colors = sns.color_palette(palette, n_colors=len(uniq_nodes))\n",
    "    solid_color_map = dict(zip(uniq_nodes, colors.as_hex()))\n",
    "    node_colors = list(pd.Series(node_labels).map(solid_color_map))\n",
    "\n",
    "# # Semi-transparent Create Colors for Links (Link Color Same As Source Node)\n",
    "    alpha_colors = list([add_alpha(color, alpha) for color in colors])\n",
    "    alpha_color_map = dict(zip(uniq_nodes, alpha_colors))\n",
    "\n",
    "# # Map (possible duplicate) node_labels to unique node_ids\n",
    "\n",
    "    ct.index = node_ids[:len(ct.index)]\n",
    "    ct.index.name = source_col\n",
    "    ct.columns = node_ids[-len(ct.columns):]\n",
    "\n",
    "# # Create [Source, Target, Value, Color] Matrix for Links\n",
    "    data = pd.melt(ct.reset_index(), id_vars=source_col)\n",
    "    print(data.shape)\n",
    "    print(data.columns)\n",
    "    data.columns = [source_col, target_col, 'value']\n",
    "    data = data[data['value'] > 0]\n",
    "    data['src_color'] = data[source_col].map(label_map).map(alpha_color_map)\n",
    "\n",
    "# # Send Data to Plotly Figure\n",
    "    fig = go.Figure(data=[\n",
    "        go.Sankey(node=dict(label=node_labels, color=node_colors),\n",
    "                  link=dict(source=data[source_col], target=data[target_col],\n",
    "                    value=data['value'], color=data['src_color'])\n",
    "                  )\n",
    "    ])\n",
    "\n",
    "    titre = f'{titre} : {source_col} vs. {target_col}'\n",
    "    fig.update_layout(title_text=titre, title_x=0.5, font_size=font_size)\n",
    "\n",
    "    if to_image:\n",
    "# # Requires Package 'Kaleido'\n",
    "        w, h = figsize\n",
    "        fig_name = sanitize(titre)\n",
    "        filename = f'{IMAGE_FOLDER}/{fig_name}.png'\n",
    "        pio.write_image(fig, file=filename, format=\"png\")\n",
    "# # Pio.Write_Image (Fig, File = Filename, Format = \"PNG\", Width = W*300, Height = H*300, Scale = 1)\n",
    "        img = fig.to_image(format=\"png\", width=w, height=h, scale=1)\n",
    "        return IPython.display.Image(img)\n",
    "    else:\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Clustering Kmeans via tsne, and Metric Ari calculation\n",
    "\n",
    "Once the scores are calculated, the procedures below allows you to view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Sklearn Import Decomposition\n",
    "def reducer_pca(features: pd.Series, n_components=0.99) -> np.ndarray:\n",
    "\n",
    "    print(f'Dimensions before PCA reduction : { features.shape}')\n",
    "    pca = decomposition.PCA(n_components=n_components)\n",
    "    if n_components >= 1:\n",
    "        param_str = f'(n_components = {n_components})'\n",
    "    else:\n",
    "        param_str = f'({n_components*100:.0f} % variance explained)'\n",
    "    feat_pca = pca.fit_transform(features)\n",
    "    print(f'Dimensions after PCA reduction {param_str} : {feat_pca.shape}')\n",
    "    return feat_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Sklearn Import Manifold\n",
    "# import time\n",
    "\n",
    "\n",
    "def reducer_tsne(features: pd.Series) -> np.ndarray:\n",
    "\n",
    "    print(f'reducer t-SNE, input shape={features.shape}')\n",
    "    time1 = time.time()\n",
    "    tsne_model = manifold.TSNE(n_components=2, perplexity=30, n_iter=2000,\n",
    "                               init='random', learning_rate=200, random_state=RANDOM_SEED)\n",
    "    X_tsne = tsne_model.fit_transform(features)\n",
    "    time2 = np.round(time.time() - time1, 0)\n",
    "    print(f'reducer t-SNE, shape ={X_tsne.shape} time : {time2}')\n",
    "    return X_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Sklearn Import Cluster, Metrics\n",
    "# import time\n",
    "\n",
    "def calc_ARI(labels_pred, labels_true=y_cat_num):\n",
    "\n",
    "    if len(labels_pred) == len(labels_true):\n",
    "        return np.round(metrics.adjusted_rand_score(labels_true, labels_pred), 4)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def calc_tsne_cluster(features, categories_=list_categories, y_cat_num_=y_cat_num, k=None):\n",
    "\n",
    "    X_tsne = reducer_tsne(features)\n",
    "\n",
    "    if k is None or not k > 0:\n",
    "        k = len(categories_)\n",
    "\n",
    "    time1 = time.time()\n",
    "# # Determination of Clusters from Data After Tsne\n",
    "    cls = cluster.KMeans(n_clusters=k, n_init=100,\n",
    "                         random_state=RANDOM_SEED)\n",
    "    cls.fit(X_tsne)\n",
    "    time2 = np.round(time.time() - time1, 0)\n",
    "    if len(y_cat_num_) == len(cls.labels_):\n",
    "        ARI = np.round(metrics.adjusted_rand_score(y_cat_num_, cls.labels_), 4)\n",
    "    else:\n",
    "        ARI = -1\n",
    "\n",
    "    print(\"ARI : \", ARI, \"time : \", time2)\n",
    "# # Addition CLS.Cluster_Centers_?\n",
    "    return ARI, X_tsne, cls.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Visualization of clusters on reduced dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "def plot_clusters_sur_2D(X_tsne_, y_cat_num_, labels_, ARI_, l_cat_=list_categories,\n",
    "                         bbox_left=-0.05, loc=1,\n",
    "                         palette='bright',\n",
    "                         titre1='Representation of products by real categories',\n",
    "                         titre2='Representation of products by clusters'):\n",
    "\n",
    "    if ARI_ is None:\n",
    "        ARI_ = calc_ARI(y_cat_num_, labels_)\n",
    "    if len(y_cat_num_) == len(labels_):\n",
    "        df_opt = conf_matrix_labels(y_cat_num_, labels_)\n",
    "# # Put the order of the clusters in the same order as the y_cat\n",
    "        labels_hue_order = list(df_opt.columns)\n",
    "        n_labels = len(labels_hue_order)\n",
    "    else:\n",
    "        labels_hue_order = None\n",
    "        n_labels = pd.Series(np.array(labels_)).nunique()\n",
    "    s_labels = pd.Series(labels_)\n",
    "\n",
    "    categ_hue_order = sorted(list(pd.Series(y_cat_num_).unique()))\n",
    "    n_categ = len(categ_hue_order)\n",
    "\n",
    "    n_max = max(n_labels, n_categ)\n",
    "    colors = sns.color_palette(palette, n_colors=n_max)\n",
    "# # print (colors)\n",
    "    labels_palette = colors[:n_labels]\n",
    "    categ_palette = colors[:n_categ]\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    scatter1 = sns.scatterplot(x=X_tsne_[:, 0], y=X_tsne_[:, 1], ax=ax1,\n",
    "                               hue=y_cat_num_, hue_order=categ_hue_order, palette=categ_palette)\n",
    "    ax1.legend(title=\"Category\", bbox_to_anchor=(bbox_left, 0.5), loc=loc)\n",
    "    plt.title(titre1)\n",
    "\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    scatter2 = sns.scatterplot(x=X_tsne_[:, 0], y=X_tsne_[:, 1], ax=ax2,\n",
    "                               hue=s_labels, hue_order=labels_hue_order, palette=labels_palette)\n",
    "    ax2.legend(loc=\"best\", title=\"Clusters\")\n",
    "    plt.title(titre2)\n",
    "    print(\"ARI : \", ARI_)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Accumulation of model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_model_score(df: pd.DataFrame = None, model_name: str = 'none', ARI: float = 0, k: int = 0, **kwargs):\n",
    "    \"\"\"\n",
    "    Add model scoring results to a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame to append to (if None, uses global df_resultats)\n",
    "        model_name: Name of the model\n",
    "        ARI: Adjusted Rand Index score\n",
    "        k: Number of clusters\n",
    "        **kwargs: Additional scoring metrics to include\n",
    "\n",
    "    Returns:\n",
    "        Updated DataFrame with new scoring results\n",
    "    \"\"\"\n",
    "    global df_resultats\n",
    "    if df is None:\n",
    "        df = df_resultats\n",
    "\n",
    "    # Create dictionary with model results\n",
    "    resultats = dict(model=model_name, ARI=ARI, k=k)\n",
    "    resultats.update(kwargs)\n",
    "\n",
    "    # Convert single row dict to DataFrame and concatenate\n",
    "    new_row = pd.DataFrame([resultats])\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # Ensure k is integer type\n",
    "    df['k'] = df['k'].astype(int)\n",
    "    return df\n",
    "\n",
    "# Initialize global results DataFrame\n",
    "df_resultats = pd.DataFrame()\n",
    "\n",
    "# Test the function\n",
    "result = add_model_score(pd.DataFrame(), model_name='test_model', optimizer='adam', k=7)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Bag-of-Words and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Try 1: Bag of Words (Bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the Bag of Words (Countvectorizer)\n",
    "\n",
    "We want to find characteristic words of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Sklearn.Feature _ extraction.text import vectorizer\n",
    "\n",
    "# Words present in minimum of 3 products\n",
    "cvect = CountVectorizer(stop_words='english', min_df=3)\n",
    "\n",
    "# We apply to the sentence created from bag-of-words with lemmatization\n",
    "feature = 'sentence_bow_lem'\n",
    "# cv_fit = cvect.fit (data_t [feature])\n",
    "\n",
    "cv_transform = cvect.fit_transform(data_T[feature])\n",
    "print(cv_transform.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix `CV_transform` represents the bag-of-words created by Countvectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example_bow():\n",
    "    print(data_T[feature][40])\n",
    "    print(data_T[feature][42])\n",
    "    print(data_T[feature][43])\n",
    "\n",
    "    df_bow = pd.DataFrame.sparse.from_spmatrix(\n",
    "        cv_transform,\n",
    "        columns=cvect.get_feature_names_out(),\n",
    "        index=data_T.index)\n",
    "    df_bow = df_bow.join(data_T[['product_name']])\n",
    "    print('Portion du bag-of-words - vectors')\n",
    "    return df_bow.loc[40:43, ['boy', 'baby', 'girl', 'grey', 'blue', 'pyjama', 'cotton', 'hair', 'product_name']]\n",
    "\n",
    "\n",
    "show_example_bow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of clusters and display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uni-grams\n",
    "\n",
    "- Define `ngram_range = (1,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_count_vectorizer = CountVectorizer(ngram_range=(1, 1), min_df=3)\n",
    "tsne_reducer = manifold.TSNE(n_components=2, perplexity=30,\n",
    "                             n_iter=2000, init='random', learning_rate=200)\n",
    "\n",
    "scores1a, labels1a, X_tsne1a, k1a = score_model(data_T, labels_true=y_cat_txt,\n",
    "                                                feature_extractor=unigram_count_vectorizer,\n",
    "                                                dimension_reducer=tsne_reducer,\n",
    "                                                kmin=4, kmax=12)\n",
    "ARI1a = calc_ARI(y_cat_txt, labels1a)\n",
    "# Add to global scores\n",
    "df_resultats = add_model_score(\n",
    "    model_name='BOW (unigrams) + TSNE', ARI=ARI1a, k=k1a)\n",
    "plot_metrics(scores1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for k = 7 to turn the labels for k = 7\n",
    "scores1a, labels1a, X_tsne1a, k1a = score_model(data_T, labels_true=y_cat_txt,\n",
    "                                                feature_extractor=unigram_count_vectorizer,\n",
    "                                                dimension_reducer=tsne_reducer,\n",
    "                                                kmin=7, kmax=7)\n",
    "\n",
    "ARI1a = calc_ARI(y_cat_txt, labels1a)\n",
    "# Add to global scores\n",
    "df_resultats = add_model_score(\n",
    "    model_name='BOW (unigrams) + TSNE', ARI=ARI1a, k=k1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_1a = plot_classification_metrics(y_cat_num, labels1a)\n",
    "plt.title(\n",
    "    f'BOW (unigrams) + TSNE - confusion matrix (ARI={ari_1a:.3f})', fontsize=14)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plot_clusters_sur_2D(\n",
    "    X_tsne1a, y_cat_txt, labels1a, ARI1a, palette='bright')\n",
    "plt.suptitle(\n",
    "    f\"Visualization of clusters (feature extraction: Bag-Of-Words(unigrams); dimension reduction: TSNE, ARI = {ARI1a:.3f}\")\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words with reduction of dimensions via PCA\n",
    "\n",
    "`Pca (n_components = 0.99)` - Drive PCA and only retain the components which explains 99% of the variance\n",
    "\n",
    "- We eliminate the words that are rare or very associated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Sklearn.decomposition Import PCA\n",
    "\n",
    "pca_reducer = decomposition.PCA(n_components=0.99)\n",
    "\n",
    "scores1b, labels1b, X_tsne1b, k1b = score_model(data_T, labels_true=y_cat_txt,\n",
    "                                                feature_extractor=unigram_count_vectorizer,\n",
    "                                                dimension_reducer=pca_reducer,\n",
    "                                                kmin=4, kmax=12)\n",
    "ARI1b = calc_ARI(y_cat_txt, labels1b)\n",
    "# Add to global scores\n",
    "df_resultats = add_model_score(\n",
    "    model_name='BOW (unigrams) + PCA', ARI=ARI1b, k=k1b)\n",
    "plot_metrics(scores1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_1b = plot_classification_metrics(y_cat_txt, labels1b)\n",
    "plt.title(\n",
    "    f'BOW (unigrams) + PCA - confusion matrix (ARI={ari_1b:.3f})', fontsize=14)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View clusters for K = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for k = 7 to turn the labels for k = 7\n",
    "scores1c, labels1c, X_tsne1c, k1c = score_model(data_T, labels_true=y_cat_txt,\n",
    "                                                feature_extractor=unigram_count_vectorizer,\n",
    "                                                dimension_reducer=pca_reducer,\n",
    "                                                kmin=7, kmax=7)\n",
    "\n",
    "ARI1c = calc_ARI(y_cat_txt, labels1c)\n",
    "# Add to global scores\n",
    "df_resultats = add_model_score(\n",
    "    model_name='BOW (unigrams) + PCA', ARI=ARI1c, k=7)\n",
    "\n",
    "fig = plot_clusters_sur_2D(\n",
    "    X_tsne1c, y_cat_txt, labels1c, ARI1c, palette='bright')\n",
    "plt.suptitle(\n",
    "    f\"Visualization of clusters (feature extraction: Bag-Of-Words; dimension reduction: PCA; ARI = {ARI1c:.3f}\")\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing dimensions only with PCA does not seem very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAG-OF-Words with reduction of dimensions via PCA + TSNE\n",
    "\n",
    "We try reduction of dimensions by PCA, followed by tsne on the reduced dimensions (reduction of dimensions in two steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_tsne_reducer_pipeline = pipeline.Pipeline(\n",
    "    steps=[('pca', pca_reducer), ('tsne', tsne_reducer)])\n",
    "\n",
    "scores1d, labels1d, X_tsne1d, k1d = score_model(data_T, labels_true=y_cat_txt,\n",
    "                                                feature_extractor=unigram_count_vectorizer,\n",
    "                                                dimension_reducer=pca_tsne_reducer_pipeline,\n",
    "                                                kmin=4, kmax=12)\n",
    "plot_metrics(scores1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_1d = plot_classification_metrics(y_cat_txt, labels1d)\n",
    "plt.title(\n",
    "    f'BOW (unigrams) PCA + TSNE - confusion matrix (ARI={ari_1d:.3f})', fontsize=14)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature reduction pca + tsne creates a better silhouette score than feature reduction with only tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise Clusters produced by PCA+Tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ARI1d = metrics.adjusted_rand_score(\n",
    "    y_cat_txt, labels1d)\n",
    "\n",
    "# Add to global scores\n",
    "df_resultats = add_model_score(\n",
    "    model_name='BOW (unigrams) + PCA + TSNE', ARI=ARI1d, k=k1d)\n",
    "\n",
    "fig = plot_clusters_sur_2D(\n",
    "    X_tsne1d, y_cat_txt, labels1d, ARI1d, palette='bright')\n",
    "plt.suptitle(\n",
    "    f'Bag-of-Words (CountVectorizer), reducer(PCA + TSNE), ARI = {ARI1d:.3f}')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_1d = plot_classification_metrics(y_cat_txt, labels1d)\n",
    "plt.title(\n",
    "    f'BOW (unigrams) + PCA + TSNE - confusion matrix (ARI={ari_1d:.3f})', fontsize=14)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_count_vectorizer = CountVectorizer(ngram_range=(2, 2), min_df=3)\n",
    "\n",
    "scores1_bigrammes, labels1_bi, X_tsne1_bi, k1_bi = score_model(data_T, labels_true=y_cat_txt,\n",
    "                                                               feature_extractor=bigram_count_vectorizer,\n",
    "                                                               dimension_reducer=tsne_reducer,\n",
    "                                                               kmin=4, kmax=12)\n",
    "ARI1_bi = calc_ARI(y_cat_txt, labels1_bi)\n",
    "print(f'Bi-grammes, ARI = {ARI1_bi}')\n",
    "\n",
    "# Add to global scores\n",
    "df_resultats = add_model_score(\n",
    "    model_name='BOW (bigrams) + TSNE', ARI=ARI1_bi, k=k1_bi)\n",
    "plot_metrics(scores1_bigrammes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_count_vectorizer = CountVectorizer(ngram_range=(2, 2), min_df=3)\n",
    "\n",
    "scores1_trigrammes, labels1_tri, X_tsne1_tri, k1_tri = score_model(data_T, labels_true=y_cat_txt,\n",
    "                                                                   feature_extractor=trigram_count_vectorizer,\n",
    "                                                                   dimension_reducer=tsne_reducer,\n",
    "                                                                   kmin=4, kmax=12)\n",
    "ARI1_tri = calc_ARI(y_cat_txt, labels1_tri)\n",
    "\n",
    "print(f'Tri-grammes, ARI = {ARI1_tri}')\n",
    "\n",
    "# Add to global scores\n",
    "df_resultats = add_model_score(\n",
    "    model_name='BOW (trigrams) + TSNE', ARI=ARI1_tri, k=k1_tri)\n",
    "plot_metrics(scores1_trigrammes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette visualization of the best clusters\n",
    "\n",
    "The best performances are for unigram_vectorizer (bag_of_words) and reduction of dimensions by tsne.\n",
    "\n",
    "We look at the homogeneity of clusters for k = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from yellowbrick.Clusive import silhouettevisualizer\n",
    "# from Sklearn Import Cluster\n",
    "\n",
    "def plot_silhouettes(df: pd.DataFrame, k_clusters=7, titre=''):\n",
    "\n",
    "    cluster_colors = sns.color_palette('nipy_spectral_r', n_colors=k_clusters)\n",
    "    estimator = cluster.KMeans(n_clusters=k_clusters, random_state=RANDOM_SEED)\n",
    "    visualizer = SilhouetteVisualizer(estimator, colors=cluster_colors)\n",
    "    visualizer.fit(df)\n",
    "# # ~ 5 minutes\n",
    "    visualizer.finalize()        # Finalize and render the figure\n",
    "    subtitre = f'Silhouette_score for {k_clusters} clusters : {visualizer.silhouette_score_:.3f}'\n",
    "    plt.suptitle(f'{titre}\\n{subtitre}', fontsize=12, y=1.05)\n",
    "    to_png(f'{titre}-{subtitre}')\n",
    "\n",
    "\n",
    "plot_silhouettes(X_tsne1a, k_clusters=7,\n",
    "                 titre='Silhouettes pour Bag-Of-Words (+ TSNE feature reduction)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_silhouettes(X_tsne1d, k_clusters=7,\n",
    "                 titre='Silhouettes pour Bag-Of-Words (PCA + TSNE feature reduction)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualizes the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CountVectorizer : \")\n",
    "print(\"-----------------\")\n",
    "\n",
    "data_vectorized = unigram_count_vectorizer.fit_transform(\n",
    "    data_T['sentence_bow_lem'])\n",
    "\n",
    "ARI1, X_tsne1, labels1 = calc_tsne_cluster(data_vectorized, k=7)\n",
    "\n",
    "# Add to global scores\n",
    "df_resultats = add_model_score(\n",
    "    model_name='BOW (unigrams) + TSNE', ARI=ARI1, k=7)\n",
    "# Addition to labels\n",
    "data_T['labels_bow'] = labels1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_clusters_sur_2D(X_tsne1, y_cat_txt, labels1, ARI1, palette='bright')\n",
    "plt.suptitle(f'Bag-of-Words (CountVectorizer), ARI = {ARI1:.3f}')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sankey_confusion_diagram(y_cat_txt, labels1,\n",
    "                              titre='Bag-of-Words (CountVectorizer)', palette='bright')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison if we do not remove advertisements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We apply to the sentence created from bag-of-words with lemmatization\n",
    "feature_ads = 'sentence_bow_lem_ads'\n",
    "\n",
    "data_ads_vectorized = unigram_count_vectorizer.fit_transform(\n",
    "    data_T[feature_ads])\n",
    "\n",
    "print(\"CountVectorizer (avec ads): \")\n",
    "print(\"-----------------\")\n",
    "ARI1_ads, X_tsne1_ads, labels1_ads = calc_tsne_cluster(\n",
    "    data_ads_vectorized, k=7)\n",
    "\n",
    "# Add to global scores\n",
    "df_resultats = add_model_score(\n",
    "    model_name='BOW (avec publicités) + TSNE', ARI=ARI1, k=7)\n",
    "\n",
    "fig = plot_clusters_sur_2D(X_tsne1_ads, y_cat_txt, labels1_ads, ARI1_ads)\n",
    "plt.suptitle(f'Bag-of-Words avec Ads (CountVectorizer), ARI = {ARI1_ads:.3f}')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Ove Metrics evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_1_ads = plot_classification_metrics(y_cat_num, pd.Series(labels1_ads))\n",
    "plt.title(\n",
    "    f'BOW avec Ads (CountVectorizer) - confusion matrix (ARI={ari_1_ads:.3f})', fontsize=14)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sankey_confusion_diagram(\n",
    "    y_cat_txt, labels1_ads, titre='Bag-of-Words avec ads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, Flipkart advertisements:\n",
    "\n",
    "- Create category markers, because only certain categories contains these advertisements\n",
    "- Creates noise that prevents distinguishing the products.\n",
    "\n",
    "We remain on descriptions without advertising which is not a property of the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup variable\n",
    "del data_ads_vectorized, data_vectorized, cv_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Try 2: TF-IDF\n",
    "\n",
    "(TERM FREQUENCY - Inverse Frequency Document)\n",
    "\n",
    "### Creation of the Word Matrix (TF-IDF)\n",
    "\n",
    "We do as for the bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Sklearn.FEATURE Extraction.Text Import TF IDF QUOTRIER\n",
    "\n",
    "# Words present in minimum of 3 products\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=3)\n",
    "\n",
    "# We apply to the sentence created from bag-of-words with lemmatization\n",
    "feature = 'sentence_bow_lem'\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(data_T[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save TF-IDF features\n",
    "\n",
    "We save the features extracted from the texts by TF-IDF to add to the features extracted from the images by VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(tfidf_vectors, 'features_tfidf', OUT_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix `CTF_Transform` Take the Vectors TF-IDF created by TFIDFFQUTORIER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf = pd.DataFrame.sparse.from_spmatrix(\n",
    "    tfidf_vectors,\n",
    "    columns=tfidf_vectorizer.get_feature_names_out(),\n",
    "    index=data_T.index).join(data_T['product_name'])\n",
    "\n",
    "\n",
    "print(data_T[feature][40])\n",
    "print(data_T[feature][42])\n",
    "print(data_T[feature][43])\n",
    "\n",
    "df_tf.loc[40:45, ['baby', 'girl', 'grey', 'blue', 'pyjama',\n",
    "                  'cotton', 'hair', 'product_name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution of the model (clustering) and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tf-idf : \")\n",
    "print(\"--------\")\n",
    "ARI2, X_tsne2, labels2 = calc_tsne_cluster(tfidf_vectors)\n",
    "# Add to global scores\n",
    "df_resultats = add_model_score(model_name='TF-IDF + TSNE', ARI=ARI2, k=7)\n",
    "# Add to the result\n",
    "data_T['labels_tfidf'] = labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_clusters_sur_2D(X_tsne2, y_cat_txt, labels2, ARI2)\n",
    "plt.suptitle(f'Bag-of-Words (TF-IDF), ARI = {ARI2:.3f}')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sankey_confusion_diagram(y_cat_txt, labels2,\n",
    "                              titre='Bag-of-Words (TF-IDF)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Ove Metrics evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_labels_transform = conf_mat_transform (y_cat_num, labels2)\n",
    "ari_2 = plot_classification_metrics(y_cat_txt, labels2)\n",
    "plt.title(f'TF-IDF - confusion matrix (ARI={ari_2:.3f})', fontsize=14)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. LDA - (Topic Modeling)\n",
    "\n",
    "-<https://www.machinelearningplus.com/nlp/topic-modeling-python-cklearn-examples/>\n",
    "\n",
    "## 6.1 Try 3: LDA - Latent Dirichlet Allocation\n",
    "\n",
    "We will try to find the topics (clusters)\n",
    "\n",
    "-From Bag-Of-Words\n",
    "- from TF-IDF\n",
    "\n",
    "Features to create:\n",
    "\n",
    "-`Topic_lda_Bow`: Categories created from 'Bag-Of-Words' Descriptions\n",
    "- `Topic_lda_tf`: Categories created from 'TF-IDF' Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Try 3a: LDA on Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer='word',\n",
    "                                   ngram_range=(1, 1),\n",
    "                                   min_df=3,                        # minimum reqd occurrences of a word\n",
    "                                   stop_words='english',             # remove stop words\n",
    "                                   lowercase=True,                   # convert all words to lowercase\n",
    "# # NUM CHARS> 3\n",
    "                                   token_pattern='[a-zA-Z0-9]{3,}',\n",
    "                                   )\n",
    "\n",
    "\n",
    "# We apply to the sentence created from bag-of-words with lemmatization\n",
    "feature = 'sentence_bow_lem'\n",
    "data_vectorized = count_vectorizer.fit_transform(data_T[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparsicity Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Materialize The Sparse Data\n",
    "data_dense = data_vectorized.todense()\n",
    "\n",
    "# Compute sparsicity = percentage of non-zero cells\n",
    "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gridsearch for the best settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Sklearn.decomposition Import latentdirichletallocation\n",
    "# from Sklearn.model_selection import gridsearchcv\n",
    "\n",
    "lda = decomposition.LatentDirichletAllocation(random_state=RANDOM_SEED,\n",
    "                                              max_iter=10,               # Max learning iterations\n",
    "                                              learning_method='online',\n",
    "                                              batch_size=128,            # n docs in each learning iter\n",
    "                                              )\n",
    "\n",
    "# This param_grid takes about 5 minutes\n",
    "# Define Search Param\n",
    "search_params = {\n",
    "    'n_components': [7, 8, 9, 10],\n",
    "    'learning_decay': [.5, .7, .9]\n",
    "# # 'Learning_method': ['batch', 'online'],\n",
    "# # Reduce the Weight of First Iterations by Increasing the Learning Offset\n",
    "# # 'Learning_offset': [2, 5, 10, 20, 50, 100],\n",
    "}\n",
    "\n",
    "# Best Params\n",
    "# param_grid = {'learning_decay': [0.9], 'learning_method': ['online'], 'learning_offset': [20]}\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params, verbose=1,\n",
    "                     cv=2, return_train_score=True)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(data_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Model Performance (Perplexity and Log-Likelihood)\n",
    "\n",
    "- higher log-likelihood\n",
    "-Lower Perplexity (= Exp (-1. \\* Log-Likelihood per Word))\n",
    "\n",
    "Note: Perplexity does not consider the context and semantic associations between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score (Higher = Better)\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "print(\"Best Log Likelihood Score: \", best_lda_model.score(data_vectorized))\n",
    "\n",
    "# Perplexity = Exp (-1. * Log-likelihood per word) (Lower = Better)\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n",
    "\n",
    "# See Model Parameters\n",
    "print(best_lda_model.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com compares LDA Model Performance Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gridscores = pd.DataFrame(model.cv_results_)\n",
    "gridscores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Matplotlib.ticher Import Maxnlocator\n",
    "def plot_lda_cross_validation(model):\n",
    "\n",
    "    gridscores = pd.DataFrame(model.cv_results_)\n",
    "\n",
    "    ax = sns.lineplot(data=gridscores, x='param_n_components',\n",
    "                      y='mean_test_score', hue='param_learning_decay', palette='tab10')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    sns.despine()\n",
    "    plt.title(\"Choosing Optimal LDA Model\")\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Log Likelyhood Scores\")\n",
    "    plt.legend(title='Learning decay', loc='best')\n",
    "\n",
    "\n",
    "plot_lda_cross_validation(model)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document - Topic Matrix\n",
    "lda_output = model.best_estimator_.transform(data_vectorized)\n",
    "# n_topics = lda_output.n_components\n",
    "n_topics = lda_output.shape[1]\n",
    "# Column Names\n",
    "topics = [f'Topic {i}' for i in range(0, n_topics)]\n",
    "# Make the pandas dataframe\n",
    "df_topics = pd.DataFrame(np.round(lda_output, 2),\n",
    "                         columns=topics,\n",
    "                         index=data_T.index)\n",
    "\n",
    "# Get Dominant Topic for Each Document\n",
    "df_topics['pred_topic'] = np.argmax(df_topics.values, axis=1)\n",
    "df_data_topics = df_topics.join(data_T[['product_name', 'categ_level_1']])\n",
    "df_data_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_data_topics['pred_topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicted Topic and Level 1 comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.crosstab (df_data_topics ['Categ_level_1'],\n",
    "# df_data_topics ['pred_topic'])\n",
    "conf_matrix_labels(df_data_topics['categ_level_1'],\n",
    "                   df_topics['pred_topic']).style.background_gradient(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ARI3a = calc_ARI(df_data_topics['categ_level_1'], df_data_topics['pred_topic'])\n",
    "print(ARI3a)\n",
    "# Add to global scores\n",
    "df_resultats = add_model_score(\n",
    "    model_name='LDA(BOW) topics', ARI=ARI3a, k=n_topics)\n",
    "data_T['labels_LDA1'] = df_data_topics['pred_topic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics - LDA (Bow) evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_3a = plot_classification_metrics(y_cat_txt, data_T['labels_LDA1'])\n",
    "plt.title(\n",
    "    f'LDA(BOW) topics - confusion matrix (ARI={ari_3a:.3f})', fontsize=14)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tsne1 are the reduced dimensions of the variable \"data_vectorized\", already calculated Above\n",
    "# Ari1, x_tsne1, labels1 = calc_tsne_cluster (data_vectorized, k = 7)\n",
    "fig = plot_clusters_sur_2D(X_tsne1, y_cat_txt,\n",
    "                           data_T['labels_LDA1'], ARI3a)\n",
    "fig.axes[1].set_title('Representation des produits par topic LDA')\n",
    "plt.suptitle(f'LDA sur Bag-of-Words, ARI = {ARI3a:.3f}')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_words(model, words, nb_words=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -nb_words - 1: -1]\n",
    "        top_features = [words[i] for i in top_features_ind]\n",
    "# # Weights = Topic [Top_features_ind]\n",
    "# # For n in Range (nb_words):\n",
    "# # top_features [n] = f '{top_features [n]} [{weights [n]:.0F}] '\n",
    "        text = \", \".join(top_features)\n",
    "        print(f'Topic {idx+1}: {text}')\n",
    "\n",
    "\n",
    "get_topic_words(model.best_estimator_, cvect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topic_words(model, topics, titre='', n_top_words=10, cmap='nipy_spectral'):\n",
    "\n",
    "    nb_topics = len(model.components_)\n",
    "    colors = sns.color_palette(cmap, n_colors=nb_topics).as_hex()\n",
    "    n_cols = 4\n",
    "    n_rows = nb_topics // n_cols + (nb_topics % n_cols > 0)\n",
    "    _ = plt.figure(figsize=(n_cols*4, n_rows*3))\n",
    "# # For n, Category in Enmerate (Categories):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        ax = plt.subplot(n_rows, n_cols, topic_idx + 1)\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1: -1]\n",
    "        top_features = [topics[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "        ax.barh(top_features, weights, height=0.7, color=colors[topic_idx])\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 16})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n",
    "        ax.grid(visible=False)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.suptitle(titre, fontsize=20, y=1.02)\n",
    "\n",
    "\n",
    "plot_topic_words(model=model.best_estimator_, topics=cvect.get_feature_names_out(),\n",
    "                 titre='LDA sur bag-of-words, top 10 mots pour chaque topic')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words_from_data(df=data_T, feature='sentence_bow_lem', label_col='labels_LDA1',\n",
    "                             titre='', label_prefix='cluster', nb_words=10, cmap='nipy_spectral'):\n",
    "    label_names = df[label_col].unique()\n",
    "    nb_labels = len(label_names)\n",
    "    colors = sns.color_palette(cmap, n_colors=nb_labels, desat=0.5).as_hex()\n",
    "    n_cols = 4\n",
    "    n_rows = nb_labels // n_cols + (nb_labels % n_cols > 0)\n",
    "    _ = plt.figure(figsize=(n_cols*4, n_rows*3))\n",
    "    for idx, label in enumerate(sorted(label_names)):\n",
    "        label_data = df[df[label_col] == label]\n",
    "        freq_words = (get_corpus_freq(label_data[feature], nb=nb_words)\n",
    "                      .to_frame('count')\n",
    "                      .rename_axis('word')\n",
    "                      .reset_index()\n",
    "                      )\n",
    "# # print (f \"topic {label}: {freq_words ['word']. Tolist ()}\")\n",
    "        ax = plt.subplot(n_rows, n_cols, idx + 1)\n",
    "        ax.barh(data=freq_words, y='word', width='count',\n",
    "                height=0.7, color=colors[idx])\n",
    "        label_title = f'{label_prefix} {label}' if len(\n",
    "            str(label)) < 3 else label\n",
    "        ax.set_title(label_title, fontdict={\"fontsize\": 16})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n",
    "        ax.grid(visible=False)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.suptitle(titre, fontsize=20, y=1.02)\n",
    "\n",
    "\n",
    "plot_top_words_from_data(\n",
    "    data_T, feature='sentence_bow_lem', label_col='labels_LDA1',\n",
    "    label_prefix='topic', titre='Top mots par fréquence dans chaque topic (LDA, Bag-of-Words)')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordclouds_by_categ(\n",
    "    data_T, feature='sentence_bow_lem', categ_col='labels_LDA1', nb=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Try 3b: LDA on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Sklearn.FEATURE Extraction.Text Import TF IDF QUOTRIER\n",
    "# from Sklearn.decomposition Import latentdirichletallocation\n",
    "# from Sklearn.model_selection import gridsearchcv\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',\n",
    "                                   min_df=3,                        # minimum reqd occurrences of a word\n",
    "                                   stop_words='english',             # remove stop words\n",
    "                                   lowercase=True,                   # convert all words to lowercase\n",
    "# # NUM CHARS> 3\n",
    "                                   token_pattern='[a-zA-Z0-9]{3,}',\n",
    "                                   )\n",
    "\n",
    "\n",
    "# We apply to the Sentences Created From Bag-Of Words With Lemmatization\n",
    "feature = 'sentence_bow_lem'\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(data_T[feature])\n",
    "\n",
    "lda = decomposition.LatentDirichletAllocation(random_state=RANDOM_SEED,\n",
    "                                              max_iter=10,               # Max learning iterations\n",
    "                                              learning_method='online',\n",
    "                                              batch_size=128,            # n docs in each learning iter\n",
    "                                              )\n",
    "\n",
    "# This param_grid takes about 5 minutes\n",
    "# Define Search Param\n",
    "search_params = {\n",
    "    'n_components': [7, 8, 9, 10],\n",
    "    'learning_decay': [.5, .7, .9]\n",
    "# # 'Learning_method': ['batch', 'online'],\n",
    "# # Reduce the Weight of First Iterations by Increasing the Learning Offset\n",
    "# # 'Learning_offset': [2, 5, 10, 20, 50, 100],\n",
    "}\n",
    "\n",
    "# Best Params\n",
    "# param_grid = {'learning_decay': [0.9], 'learning_method': ['online'], 'learning_offset': [20]}\n",
    "\n",
    "# Init Grid Search Class\n",
    "model = GridSearchCV(lda, param_grid=search_params, verbose=1,\n",
    "                     cv=2, return_train_score=True)\n",
    "\n",
    "# Do the Grid Search\n",
    "model.fit(tfidf_vectors)\n",
    "\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lda_cross_validation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model: decomposition.LatentDirichletAllocation = model.best_estimator_\n",
    "lda_output = lda_model.transform(tfidf_vectors)\n",
    "\n",
    "# nb_topics = lda_output.shape [1]\n",
    "nb_topics = lda_model.components_.shape[0]\n",
    "\n",
    "# Column Names\n",
    "topics = [f'Topic {i}' for i in range(0, nb_topics)]\n",
    "df_topics = pd.DataFrame(np.round(lda_output, 2),\n",
    "                         columns=topics, index=data_T.index)\n",
    "# Get Dominant Topic for Each Document\n",
    "df_topics['pred_topic'] = np.argmax(df_topics.values, axis=1)\n",
    "print(df_topics.head())\n",
    "\n",
    "df_data_topics = data_T.join(df_topics)\n",
    "df_data_topics['pred_topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_true = cat_encoder.fit_transform(df_data_topics['categ_level_1'])\n",
    "labels_pred = df_data_topics['pred_topic']\n",
    "data_T['labels_LDA2'] = labels_pred\n",
    "data_T['labels_LDA2'] = data_T['labels_LDA2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_T.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_labels(data_T['categ_level_1'],\n",
    "                   data_T['labels_LDA2']).style.background_gradient(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI3b = calc_ARI(df_data_topics['categ_level_1'], labels_pred)\n",
    "print(ARI3b)\n",
    "# Add to global scores\n",
    "df_resultats = add_model_score(\n",
    "    model_name='LDA(TF_IDF) topics', ARI=ARI3b, k=nb_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_T['categ_level_1'].nunique())\n",
    "print(data_T['labels_LDA2'].nunique())\n",
    "\n",
    "ari_3b2 = plot_classification_metrics(\n",
    "    data_T['categ_level_1'], data_T['labels_LDA2'])\n",
    "plt.title(\n",
    "    f'LDA (TF-IDF) topics - confusion matrix (ARI={ari_3b2:.3f})', fontsize=14)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI3b2, X_tsne3b, labels3b = calc_tsne_cluster(tfidf_vectors)\n",
    "print(ARI3b2)\n",
    "\n",
    "fig = plot_clusters_sur_2D(X_tsne3b, y_cat_txt,\n",
    "                           data_T['labels_LDA2'], ARI3b)\n",
    "fig.axes[1].set_title('Representation des produits par topic LDA')\n",
    "plt.suptitle(f'LDA sur TF-IDF, ARI = {ARI3b:.3f}')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_sankey_confusion_diagram(y_cat_txt, labels3b,\n",
    "                              titre=f'LDA sur TF-IDF, ARI = {ARI3b:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_topic_words(model=model.best_estimator_,\n",
    "                words=tfidf_vectorizer.get_feature_names_out())\n",
    "plot_topic_words(model=model.best_estimator_, topics=tfidf_vectorizer.get_feature_names_out(),\n",
    "                 titre='LDA sur bag-of-words, top 10 mots pour chaque topic')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_words_from_data(\n",
    "    data_T, feature='sentence_bow_lem', label_col='labels_LDA2', nb_words=10)\n",
    "plt.suptitle('LDA sur TF-IDF, top 10 mots pour chaque topic')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordclouds_by_categ(\n",
    "    data_T, feature='sentence_bow_lem', categ_col='labels_LDA2', nb=10)\n",
    "plt.suptitle(\n",
    "    'LDA on TF-IDF, 10 most frequent words in the product descriptions of each topic')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Sklearn.decomposition Import latentdirichletallocation\n",
    "\n",
    "def predict_topic(text,\n",
    "                  preprocessor=transform_bow_lem_fct,\n",
    "                  feature_extractor=tfidf_vectorizer,\n",
    "                  dimension_reducer=best_lda_model\n",
    "                  ):\n",
    "\n",
    "# # Step 1: Clean Text and Convert to Words\n",
    "    mytext_2 = transform_bow_lem_fct(text)\n",
    "    print(mytext_2)\n",
    "\n",
    "# # Step 2: Extract features\n",
    "    if feature_extractor:\n",
    "# # Try:\n",
    "# # feature_ extractor.check_is_fitted ()\n",
    "        mytext_3 = feature_extractor.transform([mytext_2])\n",
    "# # Except:\n",
    "\n",
    "# # Return 'feature_ extractor is not fitted'\n",
    "\n",
    "# # Step 3: reduce dimensions/predict transform\n",
    "    if isinstance(dimension_reducer, decomposition.LatentDirichletAllocation):\n",
    "        topic_probability_scores = dimension_reducer.transform(mytext_3)\n",
    "        topic = list(np.argmax(topic_probability_scores, axis=1))\n",
    "\n",
    "        return topic, topic_probability_scores\n",
    "\n",
    "\n",
    "print(predict_topic('analog watch battery'))\n",
    "print(predict_topic('soap cream massage'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Word2w\n",
    "\n",
    "Word2vec is based on ID\n",
    "\n",
    "- <https://www.tensorflow.org/tutorials/text/word2 dear>\n",
    "\n",
    "## 7.1 Try 4: Word2 with lemmatized sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# from Tensorflow.keras.models Import Model\n",
    "# For Word2W\n",
    "# Import Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the Word2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length phrases\n",
    "_ = calc_length_bow(data_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_size = 300\n",
    "w2v_window = 5\n",
    "w2v_min_count = 1\n",
    "w2v_epochs = 100\n",
    "maxlen = 50  # adapt to length of sentences\n",
    "sentences = data_T['sentence_bow_lem'].to_list()\n",
    "sentences = [gensim.utils.simple_preprocess(text) for text in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation and Training of the Word2wave Model\n",
    "\n",
    "print(\"Build & train Word2Vec model ...\")\n",
    "w2v_model = gensim.models.Word2Vec(min_count=w2v_min_count, window=w2v_window,\n",
    "                                   vector_size=w2v_size,\n",
    "                                   seed=42,\n",
    "                                   workers=1)\n",
    "# workers = multiprocessing.cpu_count ())\n",
    "w2v_model.build_vocab(sentences)\n",
    "w2v_model.train(\n",
    "    sentences, total_examples=w2v_model.corpus_count, epochs=w2v_epochs)\n",
    "model_vectors = w2v_model.wv\n",
    "w2v_words = model_vectors.index_to_key\n",
    "print(\"Vocabulary size: %i\" % len(w2v_words))\n",
    "print(\"Word2Vec trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit A 2D PCA Model to the Vectors\n",
    "X = w2v_model.wv.get_normed_vectors()\n",
    "# pca = pca (n_components = 0.95)\n",
    "# RESULT = PCA.FIT_TRANSFORM (x)\n",
    "\n",
    "X_red = reducer_pca(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Sklearn.Manifold Import Tsne\n",
    "\n",
    "def plot_top_word_vectors(w2v_model_, X_, max_w=50):\n",
    "    tsne_model = manifold.TSNE(perplexity=40, n_components=2,\n",
    "                               init='pca', learning_rate=200, n_iter=1000, random_state=23)\n",
    "    X_tsne = tsne_model.fit_transform(X_[:max_w, :])\n",
    "# # Plot the T-Sne Output\n",
    "    _, ax = plt.subplots()\n",
    "    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], s=1)\n",
    "    ax.set_title('Words')\n",
    "    ax.grid(False)\n",
    "    ax.set_yticklabels([])  # Hide ticks\n",
    "    ax.set_xticklabels([])  # Hide ticks\n",
    "\n",
    "    words = list(w2v_model_.wv.index_to_key)\n",
    "    for i, word in enumerate(words):\n",
    "        if i < max_w:\n",
    "            plt.annotate(word, xy=(X_tsne[i, 0], X_tsne[i, 1]))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_top_word_vectors(w2v_model, X, max_w=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['baby', 'soap', 'laptop', 'showpiece', 'curtain', 'kitchen', 'watch']\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "for word in keys:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    for similar_word, _ in w2v_model.wv.most_similar(word, topn=10):\n",
    "        words.append(similar_word)\n",
    "        embeddings.append(w2v_model.wv[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)\n",
    "\n",
    "tsne_model_en_2d = manifold.TSNE(\n",
    "    perplexity=30, n_components=2, init='pca', learning_rate=200, n_iter=3500, random_state=32)\n",
    "embedding_clusters = np.array(embedding_clusters)\n",
    "n, m, k = embedding_clusters.shape\n",
    "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(\n",
    "    embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "\n",
    "\n",
    "def tsne_plot_similar_words(labels, embedding_clusters, word_clusters, a=0.7):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = sns.color_palette('Dark2', n_colors=len(labels)).as_hex()\n",
    "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "        x = embeddings[:, 0]\n",
    "        y = embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label, s=1)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, color=color, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=20)\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "# tsne_plot_similar_words (Keys, Embeddings_en_2d, Word_Clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_w = 70\n",
    "ARI4a, X_tsne4a, labels4a = calc_tsne_cluster(X[:max_w, :])\n",
    "print(ARI4a)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "sns.scatterplot(x=X_tsne4a[:, 0], y=X_tsne4a[:, 1],\n",
    "                hue=labels4a, palette='Dark2', s=1, ax=ax, alpha=0.2, legend=None)\n",
    "\n",
    "colors = sns.color_palette('Dark2', n_colors=len(labels)).as_hex()\n",
    "words = list(w2v_model.wv.index_to_key)[:max_w]\n",
    "i = 0\n",
    "\n",
    "for word, label in zip(words, labels4a):\n",
    "    plt.annotate(word, color=colors[label], alpha=0.8, size=20,\n",
    "                 ha='center', va='center',\n",
    "                 xy=(X_tsne4a[i, 0], X_tsne4a[i, 1]))\n",
    "    i += 1\n",
    "\n",
    "plt.grid(False)\n",
    "plt.title('Representation des top mots par cluster', fontsize=24)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model, max_w=70):\n",
    "\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "# # for word in model.wv:\n",
    "# # Tokens.Append (Model.wv.get_vector (Word))\n",
    "# # Labels.Append (Word.index_to_Key)\n",
    "\n",
    "    tokens_all = model.wv.get_normed_vectors()\n",
    "    labels_all = model.wv.index_to_key\n",
    "\n",
    "    tokens = tokens_all[:max_w]\n",
    "    labels = labels_all[:max_w]\n",
    "\n",
    "    tsne_model = manifold.TSNE(\n",
    "        perplexity=40, n_components=2, learning_rate=200, n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    clf = cluster.KMeans(n_clusters=7)\n",
    "    clf.fit(new_values)\n",
    "    label_color_id = clf.labels_\n",
    "    colors = sns.color_palette('Dark2', n_colors=7)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i], y[i], color=colors[label_color_id[i]], s=1)\n",
    "        plt.annotate(labels[i],\n",
    "                     color=colors[label_color_id[i]],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     fontsize=14,\n",
    "                     ha='center',\n",
    "                     va='center')\n",
    "    plt.grid(False)\n",
    "# # Plt.show ()\n",
    "\n",
    "# # Add the word to the groups and focus on specific sets.\n",
    "\n",
    "\n",
    "tsne_plot(w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence preparation (tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fit Tokenizer ...\")\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "x_sentences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    tokenizer.texts_to_sequences(sentences),\n",
    "    maxlen=maxlen, padding='post')\n",
    "\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "print(\"Number of unique words: %i\" % num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the Embedding Matrix\n",
    "\n",
    "print(\"Create Embedding matrix ...\")\n",
    "# w2v_size = 300\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, w2v_size))\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for word, idx in word_index.items():\n",
    "    i += 1\n",
    "    if word in w2v_words:\n",
    "        j += 1\n",
    "        embedding_vector = model_vectors[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = model_vectors[word]\n",
    "\n",
    "word_rate = np.round(j/i, 4)\n",
    "print(\"Word embedding rate : \", word_rate)\n",
    "print(\"Embedding matrix: %s\" % str(embedding_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, key in enumerate(word_index.keys()):\n",
    "    if i < 5:\n",
    "        print(f'{key} : {word_index[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the Model\n",
    "\n",
    "# Input = tf.keras.layers.input (shape = (len (x_sentencies), maxlen), dtype = 'float64')\n",
    "word_input = tf.keras.layers.Input(shape=(maxlen,), dtype='float64')\n",
    "word_embedding = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                                           output_dim=w2v_size,\n",
    "                                           weights=[embedding_matrix],\n",
    "                                           input_length=maxlen)(word_input)\n",
    "word_vec = tf.keras.layers.GlobalAveragePooling1D()(word_embedding)\n",
    "embed_model = tf.keras.models.Model([word_input], word_vec)\n",
    "\n",
    "embed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed_model.predict(x_sentences)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI4, X_tsne4, labels4 = calc_tsne_cluster(embeddings)\n",
    "# Add to global scores\n",
    "df_resultats = add_model_score(\n",
    "    model_name='Word2Vec', ARI=ARI4, k=pd.Series(labels4).nunique())\n",
    "data_T['labels_W2V'] = labels4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Ove Metrics evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_4 = plot_classification_metrics(data_T['categ_level_1'], labels4)\n",
    "plt.title(f'Word2Vec - confusion matrix (ARI={ari_4:.3f})', fontsize=14)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_clusters_sur_2D(X_tsne4, y_cat_txt, labels4, ARI4)\n",
    "plt.suptitle(f'Word2Vec, ARI = {ARI4:.3f}')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Keras Model\n",
    "del embed_model\n",
    "del embeddings\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from Transformers Import Autotokenizer\n",
    "# import bone\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.preprocessing.text import tokenizer\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "print(tf.keras.__version__)\n",
    "\n",
    "# Bert\n",
    "# from Transformers Import *\n",
    "\n",
    "os.environ[\"TF_KERAS\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(\n",
    "    tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Bert - common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# Import Transformers\n",
    "\n",
    "# Sentences Preparation Function\n",
    "def bert_inp_fct(sentences, bert_tokenizer, max_length):\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_mask = []\n",
    "    bert_inp_tot = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        bert_inp = bert_tokenizer.encode_plus(sent,\n",
    "                                              add_special_tokens=True,\n",
    "                                              max_length=max_length,\n",
    "                                              padding='max_length',\n",
    "                                              return_attention_mask=True,\n",
    "                                              return_token_type_ids=True,\n",
    "                                              truncation=True,\n",
    "                                              return_tensors=\"tf\")\n",
    "\n",
    "        input_ids.append(bert_inp['input_ids'][0])\n",
    "        token_type_ids.append(bert_inp['token_type_ids'][0])\n",
    "        attention_mask.append(bert_inp['attention_mask'][0])\n",
    "        bert_inp_tot.append((bert_inp['input_ids'][0],\n",
    "                             bert_inp['token_type_ids'][0],\n",
    "                             bert_inp['attention_mask'][0]))\n",
    "\n",
    "    input_ids = np.asarray(input_ids)\n",
    "    token_type_ids = np.asarray(token_type_ids)\n",
    "    attention_mask = np.array(attention_mask)\n",
    "\n",
    "    return input_ids, token_type_ids, attention_mask, bert_inp_tot\n",
    "\n",
    "\n",
    "# Features Creation Function\n",
    "def feature_BERT_fct(model, model_type: str, sentences, max_length, b_size, mode='HF'):\n",
    "\n",
    "    batch_size = b_size\n",
    "    batch_size_pred = b_size\n",
    "    bert_tokenizer = transformers.AutoTokenizer.from_pretrained(model_type)\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(len(sentences)//batch_size):\n",
    "        idx = step*batch_size\n",
    "        input_ids, token_type_ids, attention_mask, bert_inp_tot = bert_inp_fct(sentences[idx:idx+batch_size],\n",
    "                                                                               bert_tokenizer, max_length)\n",
    "\n",
    "        if mode == 'HF':    # Bert HuggingFace\n",
    "            outputs = model.predict(\n",
    "                [input_ids, attention_mask, token_type_ids], batch_size=batch_size_pred)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        if mode == 'TFhub':  # Bert Tensorflow Hub\n",
    "            text_preprocessed = {\"input_word_ids\": input_ids,\n",
    "                                 \"input_mask\": attention_mask,\n",
    "                                 \"input_type_ids\": token_type_ids}\n",
    "            outputs = model(text_preprocessed)\n",
    "            last_hidden_states = outputs['sequence_output']\n",
    "\n",
    "        if step == 0:\n",
    "            last_hidden_states_tot = last_hidden_states\n",
    "            last_hidden_states_tot_0 = last_hidden_states\n",
    "        else:\n",
    "            last_hidden_states_tot = np.concatenate(\n",
    "                (last_hidden_states_tot, last_hidden_states))\n",
    "\n",
    "    features_bert = np.array(last_hidden_states_tot).mean(axis=1)\n",
    "\n",
    "    time2 = np.round(time.time() - time1, 0)\n",
    "    print(\"processing time: \", time2)\n",
    "\n",
    "    return features_bert, last_hidden_states_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Try 5: Bert Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 'Bert-Base-Uncased'\n",
    "\n",
    "- Downloads 511 MB Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Transformers Import Tfautomodel\n",
    "\n",
    "max_length = 64\n",
    "batch_size = 10\n",
    "model_type = 'bert-base-uncased'\n",
    "model = transformers.TFAutoModel.from_pretrained(model_type)\n",
    "\n",
    "sentences = data_T['sentence_dl'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of Features\n",
    "# Downloads Bert Features\n",
    "features_bert, last_hidden_states_tot = feature_BERT_fct(model, model_type, sentences,\n",
    "                                                         max_length, batch_size, mode='HF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI5, X_tsne5, labels5 = calc_tsne_cluster(features_bert)\n",
    "# Add to global scores\n",
    "df_resultats = add_model_score(\n",
    "    model_name='Bert HuggingFace', ARI=ARI5, k=pd.Series(labels5).nunique())\n",
    "data_T['labels_berthf'] = labels5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Ove Metrics evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_5 = plot_classification_metrics(data_T['categ_level_1'], labels5)\n",
    "plt.title(\n",
    "    f'Bert HuggingFace - confusion matrix (ARI={ari_5:.3f})', fontsize=14)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_sur_2D(X_tsne5, y_cat_txt, labels5, ARI5)\n",
    "plt.suptitle(f'BERT HuggingFace, ARI = {ARI5:.3f}')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Keras Model\n",
    "del model\n",
    "del features_bert\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Try 6: Bert Hub Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "# Guide on the tensorflow hub: https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
    "# Downloads 500MB of Model\n",
    "small_model_url = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
    "model_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n",
    "bert_layer = tensorflow_hub.KerasLayer(model_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data_T['sentence_dl'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 64\n",
    "batch_size = 10\n",
    "model_type = 'bert-base-uncased'\n",
    "model = bert_layer\n",
    "\n",
    "features_bert, last_hidden_states_tot = feature_BERT_fct(model, model_type, sentences,\n",
    "                                                         max_length, batch_size, mode='TFhub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI6, X_tsne6, labels6 = calc_tsne_cluster(features_bert)\n",
    "# Add to global scores\n",
    "df_resultats = add_model_score(\n",
    "    model_name='BERT Hub (base uncased)', ARI=ARI6, k=pd.Series(labels6).nunique())\n",
    "data_T['labels_berthub'] = labels6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics evaluation (Bert Uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_6 = plot_classification_metrics(data_T['categ_level_1'], labels6)\n",
    "plt.title(\n",
    "    f'Bert base uncased - confusion matrix (ARI={ari_6:.3f})', fontsize=14)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_sur_2D(X_tsne6, y_cat_txt, labels6, ARI6)\n",
    "print(f'BERT model_url : {model_url}')\n",
    "plt.suptitle(f'BERT Hub (base uncased), ARI = {ARI6:.3f}')\n",
    "\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Keras Model\n",
    "del model\n",
    "del bert_layer\n",
    "del features_bert\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Use - Universal Sentence encoder\n",
    "\n",
    "## 9.1 Try 7: Use - Universal Sentence encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# Bert\n",
    "# Import Transformers\n",
    "os.environ[\"TF_KERAS\"] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(\n",
    "    tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_hub as hub\n",
    "USE_model_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "# 1GB Model\n",
    "embed = tensorflow_hub.load(USE_model_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_USE_fct(sentences, b_size):\n",
    "    batch_size = b_size\n",
    "    time1 = time.time()\n",
    "\n",
    "    for step in range(len(sentences)//batch_size):\n",
    "        idx = step*batch_size\n",
    "        feat = embed(sentences[idx:idx+batch_size])\n",
    "\n",
    "        if step == 0:\n",
    "            features = feat\n",
    "        else:\n",
    "            features = np.concatenate((features, feat))\n",
    "\n",
    "    time2 = np.round(time.time() - time1, 0)\n",
    "    print(f'feature_USE_fct, time_taken = {time2} s')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "sentences = data_T['sentence_dl'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_USE = feature_USE_fct(sentences, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI7, X_tsne7, labels7 = calc_tsne_cluster(features_USE)\n",
    "# Add to global scores\n",
    "df_resultats = add_model_score(\n",
    "    model_name='BERT Hub (base uncased)', ARI=ARI7, k=pd.Series(labels7).nunique())\n",
    "data_T['labels_USE'] = labels7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics (USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_7 = plot_classification_metrics(data_T['categ_level_1'], labels7)\n",
    "plt.title(\n",
    "    f'Universal Sentence Encoder - confusion matrix (ARI={ari_7:.3f})', fontsize=14)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_sur_2D(X_tsne7, y_cat_txt, labels7, ARI7)\n",
    "\n",
    "USE_model_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "print(f'USE model_url : {USE_model_url}')\n",
    "plt.suptitle(f'USE - Universal Sentence Encoder, ARI = {ARI7:.3f}')\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup Keras Model\n",
    "del embed\n",
    "del features_USE\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Supervised Labeling with a simple neural network\n",
    "\n",
    "The best score (faster) was with TF-IDF\n",
    "\n",
    "We will lead to a neural network to see if we can improve the predictions\n",
    "\n",
    "Note: This model uses a vocabulary very limited to our samples, and perhaps cannot generalize to other products.\n",
    "\n",
    "If this is the case, you must replace the Tokenizer with an Embedding Layer as in the Word2we Model below.In this case, we can do the transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import sequential\n",
    "# from tensorflow.keras.layers import dense\n",
    "# from tensorflow.keras.preprocessing.text import tokenizer\n",
    "\n",
    "# from Sklearn.Preprocessing import labelencoder\n",
    "# from Sklearn.FEATURE Extraction.Text Import TF IDF QUOTRIER\n",
    "# from Sklearn.model_selection import train_test_split\n",
    "\n",
    "# Top 1000 Words Present in Minimum of 3 Products\n",
    "# tfidf_vectorizer = tfidfvectorizer (max_Features = 1000,\n",
    "# stop_words = 'English', min_df = 3)\n",
    "\n",
    "# We apply to the sentence created from bag-of-words with lemmatization\n",
    "# feature = 'sentence_bow_lem'\n",
    "# tfidf_vectors = tfidf_vectorizer.fit_transform (data_t [feature])\n",
    "\n",
    "\n",
    "# Documents = data_t ['Description']\n",
    "# Documents = data_t ['sentence_dl']\n",
    "documents = data_T['sentence_bow_lem']\n",
    "labels = data_T['categ_level_1']\n",
    "\n",
    "# train_documents, test_documents, train_labels, test_labels = train_test_split (documents, labels, stratify = labels, train_size = 0.8)\n",
    "train_documents, test_documents, train_labels, test_labels = train_test_split(\n",
    "    documents, labels, train_size=0.8)\n",
    "\n",
    "# preprcy\n",
    "# With tf -idf, we need to create a great matrix sparse of the frequencies of words\n",
    "# For this Study, it works\n",
    "# But if we had 50,000 products, Maybe the top 1000 terms is not enclic to group them\n",
    "vocab_size = 1000\n",
    "tokenize = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
    "tokenize.fit_on_texts(train_documents)\n",
    "\n",
    "# Word_Frequencies\n",
    "x_train = tokenize.texts_to_matrix(train_documents, mode='tfidf')\n",
    "x_test = tokenize.texts_to_matrix(test_documents, mode='tfidf')\n",
    "print(x_train.shape)\n",
    "\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(train_labels)\n",
    "y_train = encoder.transform(train_labels)\n",
    "y_test = encoder.transform(test_labels)\n",
    "num_labels = labels.nunique()\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    512, input_shape=(vocab_size,), activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(num_labels, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=16,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_diagnostic_learning_curves(history):\n",
    "\n",
    "    colors = sns.color_palette('tab10')\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# # Plot Loss\n",
    "    ax1 = axs[0]\n",
    "    ax1.set_title('Cross Entropy Loss')\n",
    "    ax1.plot(history.history['loss'], c=colors[0], label='train')\n",
    "    ax1.plot(history.history['val_loss'], c=colors[1], label='validate')\n",
    "    ax1.set_xlabel('epochs')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.legend()\n",
    "# # Plot accuracy\n",
    "    ax2 = axs[1]\n",
    "    ax2.set_title('Classification Accuracy')\n",
    "    ax2.plot(history.history['accuracy'], c=colors[0], label='train')\n",
    "    ax2.plot(history.history['val_accuracy'], c=colors[1], label='validate')\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel('epochs')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    fig.suptitle('Loss and accuracy evolution over epochs')\n",
    "\n",
    "\n",
    "plot_diagnostic_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame({})\n",
    "for i in range(5):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    print(prediction)\n",
    "    print(np.argmax(prediction[0]))\n",
    "    text_labels = encoder.classes_\n",
    "    predicted_label = encoder.inverse_transform(\n",
    "        [np.argmax(prediction[0]).sum()])\n",
    "    predicted_weight = np.round(100 * prediction[0].max(), 2)\n",
    "    print(test_documents.iloc[i][:50], \"...\")\n",
    "    print(f'Actual label: {test_labels.iloc[i]}')\n",
    "    print(f'Predicted label: {predicted_label} ({predicted_weight:.0f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pred_label(x_, model_, encoder_, documents_):\n",
    "    predictions = np.array(model_.predict(x_))\n",
    "    text_labels = encoder_.classes_\n",
    "\n",
    "# # print (predictions [: 4])\n",
    "    predicted_labels = []\n",
    "    predicted_weights = []\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        predicted_label = text_labels[np.argmax(prediction)]\n",
    "        predicted_labels.append(predicted_label)\n",
    "        predicted_weights.append(np.round(np.max(prediction)*100, 2))\n",
    "\n",
    "    pred_labels = pd.Series(\n",
    "        predicted_labels, index=documents_.index).rename('predicted')\n",
    "    pred_weights = pd.Series(\n",
    "        predicted_weights, index=documents_.index).rename('pred_weight')\n",
    "    return pred_labels, pred_weights\n",
    "\n",
    "\n",
    "test_pred_labels, test_pred_weights = calc_pred_label(\n",
    "    x_test, model, encoder, test_documents)\n",
    "train_pred_labels, train_pred_weights = calc_pred_label(\n",
    "    x_train, model, encoder, train_documents)\n",
    "\n",
    "print(list(test_pred_labels[:10]))\n",
    "print(list(test_labels[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI8 = calc_ARI(test_labels, test_pred_labels)\n",
    "print(ARI8)\n",
    "df_resultats = add_model_score(model_name='supervised TF-IDF', ARI=ARI8, k=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics evaluation - TF -IDF supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ari_8 = plot_classification_metrics(test_labels, test_pred_labels)\n",
    "plt.title(\n",
    "    f'TF-IDF supervisée - confusion matrix (ARI={ari_8:.3f})', fontsize=14)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_sankey_confusion_diagram(source=test_labels, target=test_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_labels(test_labels, test_pred_labels).style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MISCLASSIFIED Texts\n",
    "\n",
    "Train and test data is included to see misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = pd.concat([test_pred_labels, train_pred_labels], axis=0)\n",
    "pred_weights = pd.concat([test_pred_weights, train_pred_weights], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.concat([labels, pred_labels, pred_weights], axis=1).join(\n",
    "    documents).join(data_T['description'])\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = df_pred['categ_level_1'] != df_pred['predicted']\n",
    "print(misclassified.sum())\n",
    "df_pred[misclassified].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the image, it will be difficult for a human to classify these items.So it is not necessarily interesting to make the Learning transfer on these items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "del model\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised classification with clean Word Embeddings\n",
    "\n",
    "Note: Using only the vocabulary of the documents present, we create an 'overfit' (many biases) on the products already present.\n",
    "\n",
    "However, if we cannot classify an item automatically via Unsupervised Classification (Text+ Images), we can use this model to offer a more likely category / subcategory\n",
    "\n",
    "-https://machinelearningmaster.com/use-word-embedding-layers-deep-learning-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequences Ready for Word_Embedding Input\n",
    "x_train_for_embed = tokenize.texts_to_sequences(train_documents)\n",
    "x_test_for_embed = tokenize.texts_to_sequences(test_documents)\n",
    "\n",
    "max_words = 50\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    x_train_for_embed, maxlen=max_words)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    x_test_for_embed, maxlen=max_words)\n",
    "embedding_dim = 4\n",
    "print(x_train.shape)\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(\n",
    "    vocab_size, embedding_dim, input_length=max_words))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(7, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=16,\n",
    "                    epochs=100,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagnostic_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_labels, test_pred_weights = calc_pred_label(\n",
    "    x_test, model, encoder, test_documents)\n",
    "train_pred_labels, train_pred_weights = calc_pred_label(\n",
    "    x_train, model, encoder, train_documents)\n",
    "\n",
    "print(list(test_pred_labels[:10]))\n",
    "print(list(test_labels[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics evaluation - Word Embedding Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ari_9 = plot_classification_metrics(\n",
    "    test_labels, test_pred_labels, target_names=test_labels.unique())\n",
    "plt.title(\n",
    "    f'Word Embedding supervisée - confusion matrix (ARI={ari_9:.3f})', fontsize=14)\n",
    "to_png()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultats = add_model_score(\n",
    "    model_name='Word Embedding supervisée', ARI=ari_9, k=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_labels(test_labels, test_pred_labels).style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## 11.1 Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resultats = df_resultats.sort_values(by='ARI', ascending=False)\n",
    "df_resultats.to_csv(f'{OUT_FOLDER}/nlp_resultats.csv')\n",
    "df_resultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Recording of results\n",
    "\n",
    "Record data with product labels by each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_t.to_csv (f '{out_folder} /data_text_labelled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('OC_3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "de73adf3636becfdd28fc3f09c4b7cfc5d68803368fa5cbfa6ecb5d977c6da89"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "034f115832e14aac89103738d940cc42": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "062a49134226485e93e7c82232edccee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a5fc15f370f45a7abc3f1cab63f29d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_343c4b34e8af44f9a62b1ce3a35a627d",
      "placeholder": "​",
      "style": "IPY_MODEL_538173e0a4174f35a636a7b11f239bb8",
      "value": "model.safetensors: 100%"
     }
    },
    "10d22521c6e54069a2fa20d46a212a7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_034f115832e14aac89103738d940cc42",
      "placeholder": "​",
      "style": "IPY_MODEL_8dc5674d152f41d0ad2abc09eebbb18c",
      "value": " 466k/466k [00:00&lt;00:00, 18.9MB/s]"
     }
    },
    "11b6b5e916a14e52ba000dc4cdc85407": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "150133b1b6bd4ac8b1a0c08c59c2f773": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5670f3280eec4172b63be07776fffde5",
      "placeholder": "​",
      "style": "IPY_MODEL_d42a6ef3e09047fda7a30ec05da74a7f",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "18f086271f2349debedc6c5afaaa6c72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2095b753e98b4fcf882d4ae9fab7aa5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24e3809036e2408284d55ff2da2d0848": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "343c4b34e8af44f9a62b1ce3a35a627d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39ace8261f4949ad84dc157d0792ca5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0a5fc15f370f45a7abc3f1cab63f29d2",
       "IPY_MODEL_a2d91b4ed9ea4b4abd3b5a36f7d4a1eb",
       "IPY_MODEL_db951c01201d4f198d53938714025cc3"
      ],
      "layout": "IPY_MODEL_24e3809036e2408284d55ff2da2d0848"
     }
    },
    "3a709fc7619e404aaaef4d6ae8d5310f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3b5f75b78c154ff09ee4a53e1d0c0fe7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c546100d28e4b338dc8a4c484274b68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4241d8f97a274c449301a28703d18a54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "44160365837b476a80e33d6b086035f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bfaa06dd15bb460387de9417bfa43546",
       "IPY_MODEL_d39e6a14ff4644208970788c036c6c3c",
       "IPY_MODEL_b6c0c4e522a34312af209b4103816a8f"
      ],
      "layout": "IPY_MODEL_afff80034a0445aeaa3999f152ed27be"
     }
    },
    "538173e0a4174f35a636a7b11f239bb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5670f3280eec4172b63be07776fffde5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67bbf308bc85494a9390523b0352b4fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69c7384c916a49008b201e99899c441f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83ced5cdd0bc4b7a8916bcf97d51be4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e3b1591645c496cb8beaec64a5b5fa7",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f4eff2e9ab1e40f189d01824d4b4662d",
      "value": 48
     }
    },
    "84d8592d0b20482cb8e5bc0c9599f3d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "851a655c94ca420da3214bb726216cc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_150133b1b6bd4ac8b1a0c08c59c2f773",
       "IPY_MODEL_83ced5cdd0bc4b7a8916bcf97d51be4e",
       "IPY_MODEL_c97d6d006b894cda8a9de6854e3c9e6d"
      ],
      "layout": "IPY_MODEL_ef70d2f22c0e4759925ce9b23e821a71"
     }
    },
    "857b6f31964e4e02a978ea66e5266ed5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85f6027ae5504992837ecffc1bbc1af3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8dc5674d152f41d0ad2abc09eebbb18c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8e3b1591645c496cb8beaec64a5b5fa7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9210ab671ac646a194dba4285dc5fe83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9f3d6f536d174def85429990d05d8d81": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2be64d8c0b14a2b87b0b1c947e43e2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2d91b4ed9ea4b4abd3b5a36f7d4a1eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b23869d4293649ff8bfb496d469523c2",
      "max": 440449768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3a709fc7619e404aaaef4d6ae8d5310f",
      "value": 440449768
     }
    },
    "a6c3be571a104594bfc2e6af62a8f7a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6eba391bb234abb87bd168d684682dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b5f75b78c154ff09ee4a53e1d0c0fe7",
      "placeholder": "​",
      "style": "IPY_MODEL_11b6b5e916a14e52ba000dc4cdc85407",
      "value": "tokenizer.json: 100%"
     }
    },
    "ab0504c7619145f1bc35852e6ef2bb55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a6eba391bb234abb87bd168d684682dd",
       "IPY_MODEL_d3792635952a468fada74ae362e43fb4",
       "IPY_MODEL_10d22521c6e54069a2fa20d46a212a7e"
      ],
      "layout": "IPY_MODEL_fb25576aa8dc4408a1518ef9e674e0d5"
     }
    },
    "ab206a2da52741f497a06a6507185fe7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df657da5f0704bc186e61fd045eeebe3",
      "placeholder": "​",
      "style": "IPY_MODEL_a6c3be571a104594bfc2e6af62a8f7a3",
      "value": " 570/570 [00:00&lt;00:00, 25.7kB/s]"
     }
    },
    "afff80034a0445aeaa3999f152ed27be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b23869d4293649ff8bfb496d469523c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6c0c4e522a34312af209b4103816a8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de5b9ba445a64760bc3ed0581648078f",
      "placeholder": "​",
      "style": "IPY_MODEL_d21944ad95254f818cf886edef44f0d0",
      "value": " 232k/232k [00:00&lt;00:00, 5.90MB/s]"
     }
    },
    "bfa8df11d9a84c6eb00268f39592d5e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bfaa06dd15bb460387de9417bfa43546": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f3d6f536d174def85429990d05d8d81",
      "placeholder": "​",
      "style": "IPY_MODEL_2095b753e98b4fcf882d4ae9fab7aa5a",
      "value": "vocab.txt: 100%"
     }
    },
    "c97d6d006b894cda8a9de6854e3c9e6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd481df41cb24127b2de3d8f878f8bf9",
      "placeholder": "​",
      "style": "IPY_MODEL_857b6f31964e4e02a978ea66e5266ed5",
      "value": " 48.0/48.0 [00:00&lt;00:00, 2.34kB/s]"
     }
    },
    "cd481df41cb24127b2de3d8f878f8bf9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d21944ad95254f818cf886edef44f0d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3792635952a468fada74ae362e43fb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84d8592d0b20482cb8e5bc0c9599f3d1",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_18f086271f2349debedc6c5afaaa6c72",
      "value": 466062
     }
    },
    "d39e6a14ff4644208970788c036c6c3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c546100d28e4b338dc8a4c484274b68",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4241d8f97a274c449301a28703d18a54",
      "value": 231508
     }
    },
    "d42a6ef3e09047fda7a30ec05da74a7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db951c01201d4f198d53938714025cc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_062a49134226485e93e7c82232edccee",
      "placeholder": "​",
      "style": "IPY_MODEL_bfa8df11d9a84c6eb00268f39592d5e0",
      "value": " 440M/440M [00:02&lt;00:00, 186MB/s]"
     }
    },
    "de5b9ba445a64760bc3ed0581648078f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df657da5f0704bc186e61fd045eeebe3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e020ae0964ba4a47a9d7fbbac2917ef5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fdeed23b91bf430c834ad0b9efa31006",
       "IPY_MODEL_ecebd5b432c64785809d4403528e6cad",
       "IPY_MODEL_ab206a2da52741f497a06a6507185fe7"
      ],
      "layout": "IPY_MODEL_a2be64d8c0b14a2b87b0b1c947e43e2e"
     }
    },
    "ecebd5b432c64785809d4403528e6cad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69c7384c916a49008b201e99899c441f",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9210ab671ac646a194dba4285dc5fe83",
      "value": 570
     }
    },
    "ef70d2f22c0e4759925ce9b23e821a71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4eff2e9ab1e40f189d01824d4b4662d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fb25576aa8dc4408a1518ef9e674e0d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdeed23b91bf430c834ad0b9efa31006": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_85f6027ae5504992837ecffc1bbc1af3",
      "placeholder": "​",
      "style": "IPY_MODEL_67bbf308bc85494a9390523b0352b4fd",
      "value": "config.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
